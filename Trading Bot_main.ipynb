{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FNh0UUpkKgER"
   },
   "source": [
    "\n",
    "###Automated stock trading using FinRL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jK_Mjqc5KRUN"
   },
   "source": [
    "The algorithm is trained using Deep Reinforcement Learning (DRL) algorithms and the components of the reinforcement learning environment are:\n",
    "\n",
    "Action: The action space describes the allowed actions that the agent interacts with the environment. Normally, a ∈ A includes three actions: a ∈ {−1, 0, 1}, where −1, 0, 1 represent selling, holding, and buying one stock. Also, an action can be carried upon multiple shares. We use an action space {−k, ..., −1, 0, 1, ..., k}, where k denotes the number of shares. For example, \"Buy 10 shares of AAPL\" or \"Sell 10 shares of AAPL\" are 10 or −10, respectively\n",
    "\n",
    "Reward function: r(s, a, s′) is the incentive mechanism for an agent to learn a better action. The change of the portfolio value when action a is taken at state s and arriving at new state s', i.e., r(s, a, s′) = v′ − v, where v′ and v represent the portfolio values at state s′ and s, respectively\n",
    "\n",
    "State: The state space describes the observations that the agent receives from the environment. Just as a human trader needs to analyze various information before executing a trade, so our trading agent observes many different features to better learn in an interactive environment.\n",
    "\n",
    "Environment: Dow 30 consituents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vyPN-xFjKvfd"
   },
   "source": [
    "Install all the packages through FinRL library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.11.11\n"
     ]
    }
   ],
   "source": [
    "!python --version\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16341,
     "status": "ok",
     "timestamp": 1738147776086,
     "user": {
      "displayName": "Hamdan Zoghbor",
      "userId": "16799748073692251556"
     },
     "user_tz": -240
    },
    "id": "5puSWUvXBBBr",
    "outputId": "a0e47d99-a16e-4550-9e2b-762a914af452"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/AI4Finance-Foundation/FinRL.git\n",
      "  Cloning https://github.com/AI4Finance-Foundation/FinRL.git to /tmp/pip-req-build-22ikopmb\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/AI4Finance-Foundation/FinRL.git /tmp/pip-req-build-22ikopmb\n",
      "  Resolved https://github.com/AI4Finance-Foundation/FinRL.git to commit 8cf3cacc6f570d26b430e403ea522c8fe9e6876a\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git (from finrl==0.3.6)\n",
      "  Cloning https://github.com/AI4Finance-Foundation/ElegantRL.git to /tmp/pip-install-aomqx3rz/elegantrl_7741e2a78cfd4f7388da0a2621960940\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/AI4Finance-Foundation/ElegantRL.git /tmp/pip-install-aomqx3rz/elegantrl_7741e2a78cfd4f7388da0a2621960940\n",
      "  Resolved https://github.com/AI4Finance-Foundation/ElegantRL.git to commit 3bdc958c8e624b61a9e661832b01fef816924f61\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: alpaca-py<0.38,>=0.37 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from finrl==0.3.6) (0.37.0)\n",
      "Requirement already satisfied: alpaca-trade-api<4,>=3 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from finrl==0.3.6) (3.2.0)\n",
      "Requirement already satisfied: ccxt<4,>=3 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from finrl==0.3.6) (3.1.60)\n",
      "Requirement already satisfied: exchange-calendars<5,>=4 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from finrl==0.3.6) (4.9)\n",
      "Requirement already satisfied: jqdatasdk<2,>=1 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from finrl==0.3.6) (1.9.7)\n",
      "Requirement already satisfied: pyfolio<0.10,>=0.9 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from finrl==0.3.6) (0.9.2)\n",
      "Requirement already satisfied: pyportfolioopt<2,>=1 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from finrl==0.3.6) (1.5.6)\n",
      "Requirement already satisfied: ray<3,>=2 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (2.42.0)\n",
      "Requirement already satisfied: scikit-learn<2,>=1 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from finrl==0.3.6) (1.6.1)\n",
      "Requirement already satisfied: selenium<5,>=4 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from finrl==0.3.6) (4.28.1)\n",
      "Requirement already satisfied: stable-baselines3>=2.0.0a5 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (2.6.0a0)\n",
      "Requirement already satisfied: stockstats<0.6,>=0.5 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from finrl==0.3.6) (0.5.4)\n",
      "Requirement already satisfied: webdriver-manager<5,>=4 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from finrl==0.3.6) (4.0.2)\n",
      "Requirement already satisfied: wrds<4,>=3 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from finrl==0.3.6) (3.2.0)\n",
      "Requirement already satisfied: yfinance<0.3,>=0.2 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from finrl==0.3.6) (0.2.54)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.3 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.6) (1.0.3)\n",
      "Requirement already satisfied: pandas>=1.5.3 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.6) (2.2.3)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.0.3 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.6) (2.10.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.30.0 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.6) (2.32.3)\n",
      "Requirement already satisfied: sseclient-py<2.0.0,>=1.7.2 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.6) (1.8.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.6) (10.4)\n",
      "Requirement already satisfied: numpy>=1.11.1 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (1.26.4)\n",
      "Requirement already satisfied: urllib3<2,>1.24 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (1.26.20)\n",
      "Requirement already satisfied: websocket-client<2,>=0.56.0 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (1.8.0)\n",
      "Requirement already satisfied: aiohttp<4,>=3.8.3 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (3.11.11)\n",
      "Requirement already satisfied: PyYAML==6.0.1 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (6.0.1)\n",
      "Requirement already satisfied: deprecation==2.1.0 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (2.1.0)\n",
      "Requirement already satisfied: packaging in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from deprecation==2.1.0->alpaca-trade-api<4,>=3->finrl==0.3.6) (23.2)\n",
      "Requirement already satisfied: setuptools>=60.9.0 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from ccxt<4,>=3->finrl==0.3.6) (75.8.0)\n",
      "Requirement already satisfied: certifi>=2018.1.18 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from ccxt<4,>=3->finrl==0.3.6) (2025.1.31)\n",
      "Requirement already satisfied: cryptography>=2.6.1 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from ccxt<4,>=3->finrl==0.3.6) (44.0.0)\n",
      "Requirement already satisfied: aiodns>=1.1.1 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from ccxt<4,>=3->finrl==0.3.6) (3.2.0)\n",
      "Requirement already satisfied: yarl>=1.7.2 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from ccxt<4,>=3->finrl==0.3.6) (1.18.3)\n",
      "Requirement already satisfied: pyluach in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from exchange-calendars<5,>=4->finrl==0.3.6) (2.2.0)\n",
      "Requirement already satisfied: toolz in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from exchange-calendars<5,>=4->finrl==0.3.6) (1.0.0)\n",
      "Requirement already satisfied: tzdata in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from exchange-calendars<5,>=4->finrl==0.3.6) (2025.1)\n",
      "Requirement already satisfied: korean_lunar_calendar in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from exchange-calendars<5,>=4->finrl==0.3.6) (0.3.1)\n",
      "Requirement already satisfied: six in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from jqdatasdk<2,>=1->finrl==0.3.6) (1.17.0)\n",
      "Requirement already satisfied: SQLAlchemy>=1.2.8 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from jqdatasdk<2,>=1->finrl==0.3.6) (2.0.37)\n",
      "Requirement already satisfied: pymysql>=0.7.6 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from jqdatasdk<2,>=1->finrl==0.3.6) (1.1.1)\n",
      "Requirement already satisfied: thriftpy2!=0.5.1,>=0.3.9 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from jqdatasdk<2,>=1->finrl==0.3.6) (0.5.2)\n",
      "Requirement already satisfied: ipython>=3.2.3 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from pyfolio<0.10,>=0.9->finrl==0.3.6) (8.32.0)\n",
      "Requirement already satisfied: matplotlib>=1.4.0 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from pyfolio<0.10,>=0.9->finrl==0.3.6) (3.10.0)\n",
      "Requirement already satisfied: pytz>=2014.10 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from pyfolio<0.10,>=0.9->finrl==0.3.6) (2025.1)\n",
      "Requirement already satisfied: scipy>=0.14.0 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from pyfolio<0.10,>=0.9->finrl==0.3.6) (1.12.0)\n",
      "Requirement already satisfied: seaborn>=0.7.1 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from pyfolio<0.10,>=0.9->finrl==0.3.6) (0.13.2)\n",
      "Requirement already satisfied: empyrical>=0.5.0 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from pyfolio<0.10,>=0.9->finrl==0.3.6) (0.5.5)\n",
      "Requirement already satisfied: cvxpy>=1.1.19 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from pyportfolioopt<2,>=1->finrl==0.3.6) (1.6.0)\n",
      "Requirement already satisfied: ecos<3.0.0,>=2.0.14 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from pyportfolioopt<2,>=1->finrl==0.3.6) (2.0.14)\n",
      "Requirement already satisfied: plotly<6.0.0,>=5.0.0 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from pyportfolioopt<2,>=1->finrl==0.3.6) (5.24.1)\n",
      "Requirement already satisfied: click>=7.0 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.6) (8.1.8)\n",
      "Requirement already satisfied: filelock in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.6) (3.17.0)\n",
      "Requirement already satisfied: jsonschema in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.6) (4.23.0)\n",
      "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.6) (5.29.3)\n",
      "Requirement already satisfied: aiosignal in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.6) (1.3.2)\n",
      "Requirement already satisfied: frozenlist in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.6) (1.5.0)\n",
      "Requirement already satisfied: aiohttp-cors in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (0.7.0)\n",
      "Requirement already satisfied: colorful in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (0.5.6)\n",
      "Requirement already satisfied: opencensus in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (0.11.4)\n",
      "Requirement already satisfied: prometheus-client>=0.7.1 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (0.21.1)\n",
      "Requirement already satisfied: smart-open in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (7.1.0)\n",
      "Requirement already satisfied: virtualenv!=20.21.1,>=20.0.24 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (20.29.1)\n",
      "Requirement already satisfied: py-spy>=0.2.0 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (0.4.0)\n",
      "Requirement already satisfied: grpcio>=1.42.0 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (1.70.0)\n",
      "Requirement already satisfied: tensorboardX>=1.9 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (2.6.2.2)\n",
      "Requirement already satisfied: pyarrow>=9.0.0 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (19.0.0)\n",
      "Requirement already satisfied: fsspec in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (2025.2.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from scikit-learn<2,>=1->finrl==0.3.6) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from scikit-learn<2,>=1->finrl==0.3.6) (3.5.0)\n",
      "Requirement already satisfied: trio~=0.17 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from selenium<5,>=4->finrl==0.3.6) (0.28.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from selenium<5,>=4->finrl==0.3.6) (0.11.1)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from selenium<5,>=4->finrl==0.3.6) (4.12.2)\n",
      "Requirement already satisfied: gymnasium<1.1.0,>=0.29.1 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (1.0.0)\n",
      "Requirement already satisfied: torch<3.0,>=2.3 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (2.6.0)\n",
      "Requirement already satisfied: cloudpickle in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (3.1.1)\n",
      "Requirement already satisfied: opencv-python in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (4.11.0.86)\n",
      "Requirement already satisfied: pygame in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (2.6.1)\n",
      "Requirement already satisfied: tensorboard>=2.9.1 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (2.18.0)\n",
      "Requirement already satisfied: psutil in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (6.1.1)\n",
      "Requirement already satisfied: tqdm in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (4.67.1)\n",
      "Requirement already satisfied: rich in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (13.9.4)\n",
      "Requirement already satisfied: ale-py>=0.9.0 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (0.10.1)\n",
      "Requirement already satisfied: pillow in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (11.1.0)\n",
      "Requirement already satisfied: python-dotenv in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from webdriver-manager<5,>=4->finrl==0.3.6) (1.0.1)\n",
      "Requirement already satisfied: psycopg2-binary<2.10,>=2.9 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from wrds<4,>=3->finrl==0.3.6) (2.9.10)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.6) (0.0.11)\n",
      "Requirement already satisfied: platformdirs>=2.0.0 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.6) (4.3.6)\n",
      "Requirement already satisfied: frozendict>=2.3.4 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.6) (2.4.6)\n",
      "Requirement already satisfied: peewee>=3.16.2 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.6) (3.17.8)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.6) (4.13.1)\n",
      "Requirement already satisfied: th in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git->finrl==0.3.6) (0.4.1)\n",
      "Requirement already satisfied: pycares>=4.0.0 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from aiodns>=1.1.1->ccxt<4,>=3->finrl==0.3.6) (4.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.6) (2.4.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.6) (25.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.6) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.6) (0.2.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from beautifulsoup4>=4.11.1->yfinance<0.3,>=0.2->finrl==0.3.6) (2.6)\n",
      "Requirement already satisfied: cffi>=1.12 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from cryptography>=2.6.1->ccxt<4,>=3->finrl==0.3.6) (1.17.1)\n",
      "Requirement already satisfied: osqp>=0.6.2 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from cvxpy>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.6) (0.6.7.post3)\n",
      "Requirement already satisfied: clarabel>=0.5.0 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from cvxpy>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.6) (0.10.0)\n",
      "Requirement already satisfied: scs>=3.2.4.post1 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from cvxpy>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.6) (3.2.7.post2)\n",
      "Requirement already satisfied: pandas-datareader>=0.2 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from empyrical>=0.5.0->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.10.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from gymnasium<1.1.0,>=0.29.1->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (0.0.4)\n",
      "Requirement already satisfied: decorator in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (3.0.50)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (2.19.1)\n",
      "Requirement already satisfied: stack_data in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (5.14.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from matplotlib>=1.4.0->pyfolio<0.10,>=0.9->finrl==0.3.6) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from matplotlib>=1.4.0->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from matplotlib>=1.4.0->pyfolio<0.10,>=0.9->finrl==0.3.6) (4.55.8)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from matplotlib>=1.4.0->pyfolio<0.10,>=0.9->finrl==0.3.6) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from matplotlib>=1.4.0->pyfolio<0.10,>=0.9->finrl==0.3.6) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from matplotlib>=1.4.0->pyfolio<0.10,>=0.9->finrl==0.3.6) (2.9.0.post0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from plotly<6.0.0,>=5.0.0->pyportfolioopt<2,>=1->finrl==0.3.6) (9.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.0.3->alpaca-py<0.38,>=0.37->finrl==0.3.6) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.0.3->alpaca-py<0.38,>=0.37->finrl==0.3.6) (2.27.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from requests<3.0.0,>=2.30.0->alpaca-py<0.38,>=0.37->finrl==0.3.6) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from requests<3.0.0,>=2.30.0->alpaca-py<0.38,>=0.37->finrl==0.3.6) (3.10)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from SQLAlchemy>=1.2.8->jqdatasdk<2,>=1->finrl==0.3.6) (3.1.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (2.1.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (3.1.3)\n",
      "Requirement already satisfied: Cython>=3.0.10 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from thriftpy2!=0.5.1,>=0.3.9->jqdatasdk<2,>=1->finrl==0.3.6) (3.0.11)\n",
      "Requirement already satisfied: ply<4.0,>=3.4 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from thriftpy2!=0.5.1,>=0.3.9->jqdatasdk<2,>=1->finrl==0.3.6) (3.11)\n",
      "Requirement already satisfied: networkx in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (3.1.5)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from sympy==1.13.1->torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (1.3.0)\n",
      "Requirement already satisfied: sortedcontainers in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from trio~=0.17->selenium<5,>=4->finrl==0.3.6) (2.4.0)\n",
      "Requirement already satisfied: outcome in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from trio~=0.17->selenium<5,>=4->finrl==0.3.6) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from trio~=0.17->selenium<5,>=4->finrl==0.3.6) (1.3.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from trio-websocket~=0.9->selenium<5,>=4->finrl==0.3.6) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from urllib3[socks]<3,>=1.26->selenium<5,>=4->finrl==0.3.6) (1.7.1)\n",
      "Requirement already satisfied: distlib<1,>=0.3.7 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from virtualenv!=20.21.1,>=20.0.24->ray[default,tune]<3,>=2->finrl==0.3.6) (0.3.9)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from jsonschema->ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.6) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from jsonschema->ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.6) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from jsonschema->ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.6) (0.22.3)\n",
      "Requirement already satisfied: opencensus-context>=0.1.3 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from opencensus->ray[default,tune]<3,>=2->finrl==0.3.6) (0.1.3)\n",
      "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from opencensus->ray[default,tune]<3,>=2->finrl==0.3.6) (2.24.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from rich->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (3.0.0)\n",
      "Requirement already satisfied: wrapt in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from smart-open->ray[default,tune]<3,>=2->finrl==0.3.6) (1.17.2)\n",
      "Requirement already satisfied: niltype<2.0,>=0.3 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from th->elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git->finrl==0.3.6) (1.0.2)\n",
      "Requirement already satisfied: pycparser in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from cffi>=1.12->cryptography>=2.6.1->ccxt<4,>=3->finrl==0.3.6) (2.22)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.6) (1.66.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.6) (1.26.0)\n",
      "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.6) (2.38.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from jedi>=0.16->ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.8.4)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (0.1.2)\n",
      "Requirement already satisfied: qdldl in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from osqp>=0.6.2->cvxpy>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.6) (0.1.7.post5)\n",
      "Requirement already satisfied: lxml in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from pandas-datareader>=0.2->empyrical>=0.5.0->pyfolio<0.10,>=0.9->finrl==0.3.6) (5.3.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from pexpect>4.3->ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.2.13)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (3.0.2)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium<5,>=4->finrl==0.3.6) (0.14.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from stack_data->ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from stack_data->ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (3.0.0)\n",
      "Requirement already satisfied: pure_eval in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from stack_data->ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.2.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.6) (5.5.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.6) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.6) (4.9)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.6) (0.6.1)\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install git+https://github.com/AI4Finance-Foundation/FinRL.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: numpy in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pandas\n",
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: C:Usersnatnaminiforge3envsfinRlScripts: command not found\n"
     ]
    }
   ],
   "source": [
    "!set PATH=%PATH%;C:\\Users\\natna\\miniforge3\\envs\\finRl\\Scripts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==1.26.4 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (1.26.4)\n",
      "Requirement already satisfied: scipy==1.12.0 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (1.12.0)\n",
      "Requirement already satisfied: scikit-learn==1.6.1 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (1.6.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from scikit-learn==1.6.1) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages (from scikit-learn==1.6.1) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install numpy==1.26.4 scipy==1.12.0 scikit-learn==1.6.1\n",
    "\n",
    "\n",
    "\n",
    "# !python --version 1.23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy version: 1.26.4\n",
      "Scipy version: 1.12.0\n",
      "Scikit-learn version: 1.6.1\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import scipy\n",
    "import sklearn\n",
    "print(\"Numpy version:\", numpy.__version__)\n",
    "print(\"Scipy version:\", scipy.__version__)\n",
    "print(\"Scikit-learn version:\", sklearn.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3JumKAPcK1t3"
   },
   "source": [
    "Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 530
    },
    "executionInfo": {
     "elapsed": 511,
     "status": "error",
     "timestamp": 1738147118819,
     "user": {
      "displayName": "Natty Metekie",
      "userId": "06571578933818431366"
     },
     "user_tz": -240
    },
    "id": "QplTAkT7I2_n",
    "outputId": "2f02a388-e14d-4826-a3b5-33624c41fbc1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages/pyfolio/pos.py:26: UserWarning: Module \"zipline.assets\" not found; mutltipliers will not be applied to position notionals.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# matplotlib.use('Agg')\n",
    "import datetime\n",
    "\n",
    "%matplotlib inline\n",
    "from finrl import config\n",
    "from finrl.meta.preprocessor.yahoodownloader import YahooDownloader\n",
    "from finrl.meta.preprocessor.preprocessors import FeatureEngineer, data_split\n",
    "from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
    "from finrl.agents.stablebaselines3.models import DRLAgent\n",
    "from finrl.plot import backtest_stats, backtest_plot, get_daily_return, get_baseline\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../FinRL-Library\")\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2SkZZUKZK46-"
   },
   "source": [
    "Create Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "uEIa8z8rI3YZ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(\"./\" + config.DATA_SAVE_DIR):\n",
    "    os.makedirs(\"./\" + config.DATA_SAVE_DIR)\n",
    "if not os.path.exists(\"./\" + config.TRAINED_MODEL_DIR):\n",
    "    os.makedirs(\"./\" + config.TRAINED_MODEL_DIR)\n",
    "if not os.path.exists(\"./\" + config.TENSORBOARD_LOG_DIR):\n",
    "    os.makedirs(\"./\" + config.TENSORBOARD_LOG_DIR)\n",
    "if not os.path.exists(\"./\" + config.RESULTS_DIR):\n",
    "    os.makedirs(\"./\" + config.RESULTS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CIWTB6spLAZC"
   },
   "source": [
    "Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12389,
     "status": "ok",
     "timestamp": 1730810015401,
     "user": {
      "displayName": "Natty Metekie",
      "userId": "06571578933818431366"
     },
     "user_tz": -240
    },
    "id": "nx66rCNoUwpf",
    "outputId": "c1bc9ad2-8788-43f7-b065-358792fef2b6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame:  (86111, 8)\n"
     ]
    }
   ],
   "source": [
    "from finrl import config_tickers\n",
    "df = YahooDownloader(start_date = '2009-01-01',\n",
    "                           end_date = '2020-09-30',\n",
    "                           ticker_list = config_tickers.DOW_30_TICKER).fetch_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AxFqg-h6Lkb7"
   },
   "source": [
    "Add technical Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43845,
     "status": "ok",
     "timestamp": 1730810059241,
     "user": {
      "displayName": "Natty Metekie",
      "userId": "06571578933818431366"
     },
     "user_tz": -240
    },
    "id": "9HRIUe4BVVgj",
    "outputId": "44e8b82f-5daf-4888-b971-c563d7679cc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully added technical indicators\n",
      "Successfully added turbulence index\n"
     ]
    }
   ],
   "source": [
    "df = FeatureEngineer(use_technical_indicator=True,\n",
    "                      tech_indicator_list = config.INDICATORS,\n",
    "                      use_turbulence=True,\n",
    "                      user_defined_feature = False).preprocess_data(df.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 348
    },
    "executionInfo": {
     "elapsed": 34076,
     "status": "ok",
     "timestamp": 1730810093313,
     "user": {
      "displayName": "Natty Metekie",
      "userId": "06571578933818431366"
     },
     "user_tz": -240
    },
    "id": "CtDEpQ-KVfjv",
    "outputId": "9fdb3922-dacc-4bb4-d1d9-2b546f814d5f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>close</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>open</th>\n",
       "      <th>volume</th>\n",
       "      <th>tic</th>\n",
       "      <th>day</th>\n",
       "      <th>macd</th>\n",
       "      <th>boll_ub</th>\n",
       "      <th>boll_lb</th>\n",
       "      <th>rsi_30</th>\n",
       "      <th>cci_30</th>\n",
       "      <th>dx_30</th>\n",
       "      <th>close_30_sma</th>\n",
       "      <th>close_60_sma</th>\n",
       "      <th>turbulence</th>\n",
       "      <th>cov_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>6.440332</td>\n",
       "      <td>7.660714</td>\n",
       "      <td>7.585000</td>\n",
       "      <td>7.622500</td>\n",
       "      <td>493729600</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0</td>\n",
       "      <td>0.118159</td>\n",
       "      <td>6.503820</td>\n",
       "      <td>5.537622</td>\n",
       "      <td>62.133204</td>\n",
       "      <td>168.772520</td>\n",
       "      <td>33.760635</td>\n",
       "      <td>6.025190</td>\n",
       "      <td>5.972406</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[0.000443059449380718, 0.00013698037579451913...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>40.265976</td>\n",
       "      <td>57.869999</td>\n",
       "      <td>56.560001</td>\n",
       "      <td>56.630001</td>\n",
       "      <td>5277400</td>\n",
       "      <td>AMGN</td>\n",
       "      <td>0</td>\n",
       "      <td>0.224876</td>\n",
       "      <td>40.696417</td>\n",
       "      <td>38.247783</td>\n",
       "      <td>52.849978</td>\n",
       "      <td>85.532074</td>\n",
       "      <td>6.350919</td>\n",
       "      <td>39.465821</td>\n",
       "      <td>39.391293</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[0.000443059449380718, 0.00013698037579451913...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>32.828991</td>\n",
       "      <td>41.099998</td>\n",
       "      <td>40.389999</td>\n",
       "      <td>40.810001</td>\n",
       "      <td>6894300</td>\n",
       "      <td>AXP</td>\n",
       "      <td>0</td>\n",
       "      <td>0.288681</td>\n",
       "      <td>33.751084</td>\n",
       "      <td>31.445586</td>\n",
       "      <td>56.779361</td>\n",
       "      <td>1.010637</td>\n",
       "      <td>11.537387</td>\n",
       "      <td>32.716404</td>\n",
       "      <td>31.134858</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[0.000443059449380718, 0.00013698037579451913...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>43.777550</td>\n",
       "      <td>56.389999</td>\n",
       "      <td>54.799999</td>\n",
       "      <td>55.720001</td>\n",
       "      <td>6186700</td>\n",
       "      <td>BA</td>\n",
       "      <td>0</td>\n",
       "      <td>0.501041</td>\n",
       "      <td>44.009583</td>\n",
       "      <td>41.898210</td>\n",
       "      <td>58.805022</td>\n",
       "      <td>81.205372</td>\n",
       "      <td>10.840906</td>\n",
       "      <td>42.319078</td>\n",
       "      <td>40.753000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[0.000443059449380718, 0.00013698037579451913...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>39.738228</td>\n",
       "      <td>59.189999</td>\n",
       "      <td>57.509998</td>\n",
       "      <td>57.650002</td>\n",
       "      <td>7325600</td>\n",
       "      <td>CAT</td>\n",
       "      <td>0</td>\n",
       "      <td>0.075448</td>\n",
       "      <td>40.134157</td>\n",
       "      <td>38.238728</td>\n",
       "      <td>55.292739</td>\n",
       "      <td>49.350630</td>\n",
       "      <td>8.534279</td>\n",
       "      <td>39.345487</td>\n",
       "      <td>38.969693</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[0.000443059449380718, 0.00013698037579451913...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date      close       high        low       open     volume   tic  \\\n",
       "0  2010-01-04   6.440332   7.660714   7.585000   7.622500  493729600  AAPL   \n",
       "1  2010-01-04  40.265976  57.869999  56.560001  56.630001    5277400  AMGN   \n",
       "2  2010-01-04  32.828991  41.099998  40.389999  40.810001    6894300   AXP   \n",
       "3  2010-01-04  43.777550  56.389999  54.799999  55.720001    6186700    BA   \n",
       "4  2010-01-04  39.738228  59.189999  57.509998  57.650002    7325600   CAT   \n",
       "\n",
       "   day      macd    boll_ub    boll_lb     rsi_30      cci_30      dx_30  \\\n",
       "0    0  0.118159   6.503820   5.537622  62.133204  168.772520  33.760635   \n",
       "1    0  0.224876  40.696417  38.247783  52.849978   85.532074   6.350919   \n",
       "2    0  0.288681  33.751084  31.445586  56.779361    1.010637  11.537387   \n",
       "3    0  0.501041  44.009583  41.898210  58.805022   81.205372  10.840906   \n",
       "4    0  0.075448  40.134157  38.238728  55.292739   49.350630   8.534279   \n",
       "\n",
       "   close_30_sma  close_60_sma  turbulence  \\\n",
       "0      6.025190      5.972406         0.0   \n",
       "1     39.465821     39.391293         0.0   \n",
       "2     32.716404     31.134858         0.0   \n",
       "3     42.319078     40.753000         0.0   \n",
       "4     39.345487     38.969693         0.0   \n",
       "\n",
       "                                            cov_list  \n",
       "0  [[0.000443059449380718, 0.00013698037579451913...  \n",
       "1  [[0.000443059449380718, 0.00013698037579451913...  \n",
       "2  [[0.000443059449380718, 0.00013698037579451913...  \n",
       "3  [[0.000443059449380718, 0.00013698037579451913...  \n",
       "4  [[0.000443059449380718, 0.00013698037579451913...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df.sort_values(['date','tic'],ignore_index=True)\n",
    "df.index = df.date.factorize()[0]\n",
    "\n",
    "cov_list = []\n",
    "# look back is one year\n",
    "lookback=252\n",
    "for i in range(lookback,len(df.index.unique())):\n",
    "  data_lookback = df.loc[i-lookback:i,:]\n",
    "  price_lookback=data_lookback.pivot_table(index = 'date',columns = 'tic', values = 'close')\n",
    "  return_lookback = price_lookback.pct_change().dropna()\n",
    "  covs = return_lookback.cov().values\n",
    "  cov_list.append(covs)\n",
    "\n",
    "df_cov = pd.DataFrame({'date':df.date.unique()[lookback:],'cov_list':cov_list})\n",
    "df = df.merge(df_cov, on='date')\n",
    "df = df.sort_values(['date','tic']).reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QupbraDPX2CX"
   },
   "source": [
    "In real life trading, the model needs to be updated periodically using rolling windows. But here I'm just cutting the data into train and trade set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1730810093313,
     "user": {
      "displayName": "Natty Metekie",
      "userId": "06571578933818431366"
     },
     "user_tz": -240
    },
    "id": "QXvQ-kfbXEMJ",
    "outputId": "dec64cc9-6b48-46e6-c337-a17022efd459"
   },
   "outputs": [],
   "source": [
    "train = data_split(df, '2009-01-01','2019-12-31')\n",
    "trade = data_split(df, '2020-01-01','2020-09-30')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YqbKpVIvW4dy"
   },
   "source": [
    "State Space and Action Space Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1730810093314,
     "user": {
      "displayName": "Natty Metekie",
      "userId": "06571578933818431366"
     },
     "user_tz": -240
    },
    "id": "2ukHtLtfWfF4",
    "outputId": "1a646bce-dd56-4b72-f39d-c638c285c5ed"
   },
   "outputs": [],
   "source": [
    "stock_dimension = len(train.tic.unique())\n",
    "state_space = 1 + 2*stock_dimension + len(config.INDICATORS)*stock_dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n",
      "291\n"
     ]
    }
   ],
   "source": [
    "print(stock_dimension)\n",
    "print(state_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "hcFRuEVUW5Mb"
   },
   "outputs": [],
   "source": [
    "# Define transaction cost lists for buying and selling stocks\n",
    "buy_cost_list = sell_cost_list = [0.001] * stock_dimension\n",
    "# Explanation: \n",
    "# - `buy_cost_list` and `sell_cost_list` represent the transaction costs as a percentage for buying and selling stocks.\n",
    "# - `[0.001] * stock_dimension` creates a list where each element is 0.001 (0.1% transaction fee), repeated for each stock.\n",
    "# - The use of `=` assigns the same list to both `buy_cost_list` and `sell_cost_list`.\n",
    "\n",
    "# Initialize the list to track the number of shares owned for each stock\n",
    "num_stock_shares = [0] * stock_dimension\n",
    "# Explanation:\n",
    "# - `num_stock_shares` is a list where each element is initialized to 0, representing that no shares are owned initially.\n",
    "# - `[0] * stock_dimension` ensures the list length matches the number of stocks (`stock_dimension`).\n",
    "\n",
    "# Create a dictionary to store environment configuration parameters\n",
    "env_kwargs = {\n",
    "    \"hmax\": 100,  # Maximum number of shares that can be bought or sold in a single transaction.\n",
    "    \"initial_amount\": 1_000_000,  # Initial cash available for the agent to trade with (e.g., $1,000,000).\n",
    "    \"num_stock_shares\": num_stock_shares,  # Initial portfolio: number of shares owned for each stock.\n",
    "    \"buy_cost_pct\": buy_cost_list,  # Transaction cost percentage for buying stocks.\n",
    "    \"sell_cost_pct\": sell_cost_list,  # Transaction cost percentage for selling stocks.\n",
    "    \"state_space\": state_space,  # Dimension of the state space (e.g., features describing the environment).\n",
    "    \"stock_dim\": stock_dimension,  # Number of stocks being traded (dimension of the stock universe).\n",
    "    \"tech_indicator_list\": config.INDICATORS,  # List of technical indicators used as features for the state space.\n",
    "    \"action_space\": stock_dimension,  # Dimension of the action space (one action per stock).\n",
    "    \"reward_scaling\": 1e-4  # Scaling factor for rewards to normalize them and improve learning stability.\n",
    "}\n",
    "# Explanation:\n",
    "# - This dictionary (`env_kwargs`) encapsulates all the necessary parameters required to initialize the stock trading environment.\n",
    "# - It includes configuration for portfolio management (e.g., `hmax`, `initial_amount`, `num_stock_shares`) and the structure of the RL problem (e.g., `state_space`, `action_space`).\n",
    "\n",
    "# Initialize the stock trading environment with the training data and configuration parameters\n",
    "e_train_gym = StockTradingEnv(df=train, **env_kwargs)\n",
    "# Explanation:\n",
    "# - `StockTradingEnv` is a custom environment class for stock trading, compliant with OpenAI Gym standards.\n",
    "# - `df=train` specifies the training data (a DataFrame containing historical stock prices and other features).\n",
    "# - `**env_kwargs` unpacks the `env_kwargs` dictionary, passing each key-value pair as an argument to the environment initializer.\n",
    "# - The environment simulates the stock trading process, enabling the RL agent to interact with it by observing states, taking actions, and receiving rewards.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1OMN6ZYNd_fC"
   },
   "source": [
    "Environment for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1730810093314,
     "user": {
      "displayName": "Natty Metekie",
      "userId": "06571578933818431366"
     },
     "user_tz": -240
    },
    "id": "Cj33JvyIX-pm",
    "outputId": "e21de6e2-91d0-4d35-a662-0d362b53f3f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv'>\n"
     ]
    }
   ],
   "source": [
    "env_train, _ = e_train_gym.get_sb_env() #get stable baseline environment for training\n",
    "print(type(env_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "c29I-eqQeE-9"
   },
   "outputs": [],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "# Set the corresponding values to 'True' for the algorithms that you want to use\n",
    "if_using_a2c = True\n",
    "if_using_ddpg = True\n",
    "if_using_ppo = True\n",
    "if_using_td3 = False\n",
    "if_using_sac = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "OKPImGqQe0yl"
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.logger import configure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uuSrLRD8eEma"
   },
   "source": [
    " Implement DRL Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current version of PyTorch:  2.6.0+cu124\n",
      "PyTorch can use GPUs!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print('Current version of PyTorch: ', torch.__version__)\n",
    "\n",
    "if torch.cuda.is_available:\n",
    "  print('PyTorch can use GPUs!')\n",
    "else:\n",
    "  print('PyTorch cannot use GPUs.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ir3Qf4BedYD"
   },
   "source": [
    "DDPG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QslI7kC_iWGz"
   },
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4503,
     "status": "ok",
     "timestamp": 1730810097812,
     "user": {
      "displayName": "Natty Metekie",
      "userId": "06571578933818431366"
     },
     "user_tz": -240
    },
    "id": "qCWbsuVKeGsT",
    "outputId": "19397fc6-5736-477d-9407-4eca82b68275"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 128, 'buffer_size': 50000, 'learning_rate': 0.001}\n",
      "Using cuda device\n",
      "Logging to results/ddpg\n"
     ]
    }
   ],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "model_ddpg = agent.get_model(\"ddpg\")\n",
    "\n",
    "if if_using_ddpg:\n",
    "  # set up logger\n",
    "  tmp_path = config.RESULTS_DIR + '/ddpg'\n",
    "  new_logger_ddpg = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_ddpg.set_logger(new_logger_ddpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 827716,
     "status": "ok",
     "timestamp": 1730810925525,
     "user": {
      "displayName": "Natty Metekie",
      "userId": "06571578933818431366"
     },
     "user_tz": -240
    },
    "id": "OLv-G6f5egU9",
    "outputId": "60b02534-683f-4fe2-d33e-df8215d9db5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 4          |\n",
      "|    fps             | 92         |\n",
      "|    time_elapsed    | 108        |\n",
      "|    total_timesteps | 10060      |\n",
      "| train/             |            |\n",
      "|    actor_loss      | 101        |\n",
      "|    critic_loss     | 2.8e+03    |\n",
      "|    learning_rate   | 0.001      |\n",
      "|    n_updates       | 9959       |\n",
      "|    reward          | -1.6278315 |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 8          |\n",
      "|    fps             | 93         |\n",
      "|    time_elapsed    | 215        |\n",
      "|    total_timesteps | 20120      |\n",
      "| train/             |            |\n",
      "|    actor_loss      | 55.4       |\n",
      "|    critic_loss     | 2.03       |\n",
      "|    learning_rate   | 0.001      |\n",
      "|    n_updates       | 20019      |\n",
      "|    reward          | -1.6278315 |\n",
      "-----------------------------------\n",
      "day: 2514, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2984876.88\n",
      "total_reward: 1984876.88\n",
      "total_cost: 1373.51\n",
      "total_trades: 40313\n",
      "Sharpe: 0.785\n",
      "=================================\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 12         |\n",
      "|    fps             | 94         |\n",
      "|    time_elapsed    | 320        |\n",
      "|    total_timesteps | 30180      |\n",
      "| train/             |            |\n",
      "|    actor_loss      | 36.7       |\n",
      "|    critic_loss     | 1.16       |\n",
      "|    learning_rate   | 0.001      |\n",
      "|    n_updates       | 30079      |\n",
      "|    reward          | -1.6278315 |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 16         |\n",
      "|    fps             | 93         |\n",
      "|    time_elapsed    | 428        |\n",
      "|    total_timesteps | 40240      |\n",
      "| train/             |            |\n",
      "|    actor_loss      | 16.7       |\n",
      "|    critic_loss     | 0.899      |\n",
      "|    learning_rate   | 0.001      |\n",
      "|    n_updates       | 40139      |\n",
      "|    reward          | -1.6278315 |\n",
      "-----------------------------------\n",
      "day: 2514, episode: 20\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2984876.88\n",
      "total_reward: 1984876.88\n",
      "total_cost: 1373.51\n",
      "total_trades: 40313\n",
      "Sharpe: 0.785\n",
      "=================================\n"
     ]
    }
   ],
   "source": [
    "trained_ddpg = agent.train_model(model=model_ddpg,\n",
    "                             tb_log_name='ddpg',\n",
    "                             total_timesteps=50000) if if_using_ddpg else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1730810925526,
     "user": {
      "displayName": "Natty Metekie",
      "userId": "06571578933818431366"
     },
     "user_tz": -240
    },
    "id": "gkfZxJDne9Ga",
    "outputId": "f1179c7a-a44a-4224-bf7a-6cbfb21548e7"
   },
   "outputs": [],
   "source": [
    "trained_ddpg.save(config.TRAINED_MODEL_DIR + \"/agent_ddpg\") if if_using_ddpg else None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GYEHIiGBiYkF"
   },
   "source": [
    "Trading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "sXDO2YkqiBTY"
   },
   "outputs": [],
   "source": [
    "e_trade_gym = StockTradingEnv(df = trade, **env_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 474,
     "status": "ok",
     "timestamp": 1730810925996,
     "user": {
      "displayName": "Natty Metekie",
      "userId": "06571578933818431366"
     },
     "user_tz": -240
    },
    "id": "sw_llsYlifvO",
    "outputId": "287fd348-832d-4fa8-9a28-fbfeb9bd40cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hit end!\n"
     ]
    }
   ],
   "source": [
    "df_account_value, df_actions = DRLAgent.DRL_prediction(\n",
    "    model=trained_ddpg,\n",
    "    environment = e_trade_gym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 261
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1730810925996,
     "user": {
      "displayName": "Natty Metekie",
      "userId": "06571578933818431366"
     },
     "user_tz": -240
    },
    "id": "LA6LdD6piy3t",
    "outputId": "27bf8a54-a0fe-4dbf-d323-a8dcfb19b361"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>account_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>2020-09-23</td>\n",
       "      <td>809889.780316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>2020-09-24</td>\n",
       "      <td>809881.904049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>2020-09-25</td>\n",
       "      <td>822188.690014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>2020-09-28</td>\n",
       "      <td>840397.895657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>2020-09-29</td>\n",
       "      <td>833575.681423</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  account_value\n",
       "183  2020-09-23  809889.780316\n",
       "184  2020-09-24  809881.904049\n",
       "185  2020-09-25  822188.690014\n",
       "186  2020-09-28  840397.895657\n",
       "187  2020-09-29  833575.681423"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_account_value.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wWjKPG7gjgZ_"
   },
   "source": [
    "Backtesting Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 485,
     "status": "ok",
     "timestamp": 1730810926475,
     "user": {
      "displayName": "Natty Metekie",
      "userId": "06571578933818431366"
     },
     "user_tz": -240
    },
    "id": "63_gK7T2pkpm",
    "outputId": "0ecc1608-4b1e-4367-9a74-bbb923f33e2f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame:  (183, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_dji = YahooDownloader(\n",
    "    start_date='2020-01-01', end_date='2020-09-30', ticker_list=[\"dji\"]\n",
    ").fetch_data()\n",
    "df_dji = df_dji[[\"date\", \"close\"]]\n",
    "fst_day = df_dji[\"close\"][0]\n",
    "dji = pd.merge(\n",
    "    df_dji[\"date\"],\n",
    "    df_dji[\"close\"].div(fst_day).mul(1000000),\n",
    "    how=\"outer\",\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    ").set_index(\"date\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1730810926475,
     "user": {
      "displayName": "Natty Metekie",
      "userId": "06571578933818431366"
     },
     "user_tz": -240
    },
    "id": "-h4JjB_ejfzz",
    "outputId": "64f57769-7f27-4e36-b971-83ef4a61a2a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============Get Backtest Results===========\n",
      "Annual return         -0.216511\n",
      "Cumulative returns    -0.166424\n",
      "Annual volatility      0.479519\n",
      "Sharpe ratio          -0.270607\n",
      "Calmar ratio          -0.496245\n",
      "Stability              0.059184\n",
      "Max drawdown          -0.436300\n",
      "Omega ratio            0.947835\n",
      "Sortino ratio         -0.369223\n",
      "Skew                        NaN\n",
      "Kurtosis                    NaN\n",
      "Tail ratio             0.956788\n",
      "Daily value at risk   -0.060929\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"==============Get Backtest Results===========\")\n",
    "now = datetime.datetime.now().strftime('%Y%m%d-%Hh%M')\n",
    "\n",
    "perf_stats_all = backtest_stats(account_value=df_account_value)\n",
    "perf_stats_all = pd.DataFrame(perf_stats_all)\n",
    "perf_stats_all.to_csv(\"./\"+config.RESULTS_DIR+\"/perf_stats_all_\"+now+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1730810926475,
     "user": {
      "displayName": "Natty Metekie",
      "userId": "06571578933818431366"
     },
     "user_tz": -240
    },
    "id": "Tb3VVI48k78N",
    "outputId": "5456ca04-b0ae-42d1-b63f-ae47e2b88498"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============Get Baseline Stats===========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame:  (188, 8)\n",
      "Annual return         -0.065199\n",
      "Cumulative returns    -0.049054\n",
      "Annual volatility      0.416030\n",
      "Sharpe ratio           0.046016\n",
      "Calmar ratio          -0.175803\n",
      "Stability              0.012240\n",
      "Max drawdown          -0.370862\n",
      "Omega ratio            1.009343\n",
      "Sortino ratio          0.062829\n",
      "Skew                        NaN\n",
      "Kurtosis                    NaN\n",
      "Tail ratio             0.860019\n",
      "Daily value at risk   -0.052339\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#baseline stats\n",
    "print(\"==============Get Baseline Stats===========\")\n",
    "baseline_df = get_baseline(\n",
    "        ticker=\"^DJI\",\n",
    "        start = '2020-01-01',\n",
    "        end = '2020-09-30')\n",
    "\n",
    "stats = backtest_stats(baseline_df, value_col_name = 'close')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3NIMIMEvk_N0"
   },
   "source": [
    "Back Test Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 509
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1730810926475,
     "user": {
      "displayName": "Natty Metekie",
      "userId": "06571578933818431366"
     },
     "user_tz": -240
    },
    "id": "78hETy-VlHqK",
    "outputId": "ff6b429f-b62f-4fc3-f4a6-eca464b79ec9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ddpg</th>\n",
       "      <th>dji</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-01-02</th>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-03</th>\n",
       "      <td>997987.774266</td>\n",
       "      <td>1.000000e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-06</th>\n",
       "      <td>998669.021350</td>\n",
       "      <td>1.002392e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-07</th>\n",
       "      <td>996850.429082</td>\n",
       "      <td>9.982119e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-08</th>\n",
       "      <td>997018.938846</td>\n",
       "      <td>1.003848e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-23</th>\n",
       "      <td>809889.780316</td>\n",
       "      <td>9.346322e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-24</th>\n",
       "      <td>809881.904049</td>\n",
       "      <td>9.364587e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-25</th>\n",
       "      <td>822188.690014</td>\n",
       "      <td>9.489818e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-28</th>\n",
       "      <td>840397.895657</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-29</th>\n",
       "      <td>833575.681423</td>\n",
       "      <td>9.587147e+05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>188 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      ddpg           dji\n",
       "date                                    \n",
       "2020-01-02  1000000.000000           NaN\n",
       "2020-01-03   997987.774266  1.000000e+06\n",
       "2020-01-06   998669.021350  1.002392e+06\n",
       "2020-01-07   996850.429082  9.982119e+05\n",
       "2020-01-08   997018.938846  1.003848e+06\n",
       "...                    ...           ...\n",
       "2020-09-23   809889.780316  9.346322e+05\n",
       "2020-09-24   809881.904049  9.364587e+05\n",
       "2020-09-25   822188.690014  9.489818e+05\n",
       "2020-09-28   840397.895657           NaN\n",
       "2020-09-29   833575.681423  9.587147e+05\n",
       "\n",
       "[188 rows x 2 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result_ddpg = df_account_value.set_index(df_account_value.columns[0])\n",
    "result = pd.DataFrame(\n",
    "    {\n",
    "        \"ddpg\": df_result_ddpg[\"account_value\"],\n",
    "        \"dji\": dji[\"close\"],\n",
    "    }\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 550,
     "status": "ok",
     "timestamp": 1730810927018,
     "user": {
      "displayName": "Natty Metekie",
      "userId": "06571578933818431366"
     },
     "user_tz": -240
    },
    "id": "3WrmcZJzpTRb",
    "outputId": "7dca1921-a179-4da1-a3bb-d256ecdf545e"
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (15,5)\n",
    "plt.figure()\n",
    "result.plot()\n",
    "plt.savefig('results_ddpg.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3sHor4GwffzI"
   },
   "source": [
    "PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1730810927018,
     "user": {
      "displayName": "Natty Metekie",
      "userId": "06571578933818431366"
     },
     "user_tz": -240
    },
    "id": "mAAyC44Ue-Uq",
    "outputId": "97fdd59a-8c4e-4e4c-9c71-de950bc9f392"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_steps': 2048, 'ent_coef': 0.01, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to results/ppo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "PPO_PARAMS = {\n",
    "    \"n_steps\": 2048,\n",
    "    \"ent_coef\": 0.01,\n",
    "    \"learning_rate\": 0.00025,\n",
    "    \"batch_size\": 128,\n",
    "}\n",
    "model_ppo = agent.get_model(\"ppo\",model_kwargs = PPO_PARAMS)\n",
    "\n",
    "if if_using_ppo:\n",
    "  # set up logger\n",
    "  tmp_path = config.RESULTS_DIR + '/ppo'\n",
    "  new_logger_ppo = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_ppo.set_logger(new_logger_ppo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2132596,
     "status": "ok",
     "timestamp": 1730813059609,
     "user": {
      "displayName": "Natty Metekie",
      "userId": "06571578933818431366"
     },
     "user_tz": -240
    },
    "id": "1kFtG_wYfoHB",
    "outputId": "a4bbbf7a-5c11-4bc9-b30d-155f852a122e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    fps             | 174       |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 11        |\n",
      "|    total_timesteps | 2048      |\n",
      "| train/             |           |\n",
      "|    reward          | 0.5651754 |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 170         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 23          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.0175762   |\n",
      "|    clip_fraction        | 0.219       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.2       |\n",
      "|    explained_variance   | -0.0143     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 5.71        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0266     |\n",
      "|    reward               | 0.021253908 |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 11.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 170         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 36          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015577299 |\n",
      "|    clip_fraction        | 0.156       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.2       |\n",
      "|    explained_variance   | 0.0134      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 6.41        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0219     |\n",
      "|    reward               | 0.045340445 |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 16.3        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 169          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 48           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0135847945 |\n",
      "|    clip_fraction        | 0.15         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -41.2        |\n",
      "|    explained_variance   | -0.0189      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 6.36         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.0163      |\n",
      "|    reward               | 2.5486352    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 15.6         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 60          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017184936 |\n",
      "|    clip_fraction        | 0.193       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.2       |\n",
      "|    explained_variance   | -0.0497     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 12.5        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0201     |\n",
      "|    reward               | -0.12768999 |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 26.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 72          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022950962 |\n",
      "|    clip_fraction        | 0.246       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | 0.0218      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 14.4        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0197     |\n",
      "|    reward               | 5.643818    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 29          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 168         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 85          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015890002 |\n",
      "|    clip_fraction        | 0.179       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | 0.00493     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 10.3        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0255     |\n",
      "|    reward               | 0.027376147 |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 22.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 168         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 97          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.01986162  |\n",
      "|    clip_fraction        | 0.192       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | -0.0155     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 9.46        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0278     |\n",
      "|    reward               | -0.34329012 |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 19.1        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 168        |\n",
      "|    iterations           | 9          |\n",
      "|    time_elapsed         | 109        |\n",
      "|    total_timesteps      | 18432      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01973222 |\n",
      "|    clip_fraction        | 0.189      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -41.4      |\n",
      "|    explained_variance   | 0.0119     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 13.7       |\n",
      "|    n_updates            | 80         |\n",
      "|    policy_gradient_loss | -0.0223    |\n",
      "|    reward               | -1.2806826 |\n",
      "|    std                  | 1.01       |\n",
      "|    value_loss           | 20.8       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 168         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 121         |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021843677 |\n",
      "|    clip_fraction        | 0.232       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.4       |\n",
      "|    explained_variance   | 0.0319      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 10.9        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0199     |\n",
      "|    reward               | -0.56681055 |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 21          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 168         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 134         |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018947866 |\n",
      "|    clip_fraction        | 0.206       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.4       |\n",
      "|    explained_variance   | 0.00954     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 9.77        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0255     |\n",
      "|    reward               | -1.8439091  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 18.2        |\n",
      "-----------------------------------------\n",
      "day: 2514, episode: 30\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3985879.98\n",
      "total_reward: 2985879.98\n",
      "total_cost: 265355.30\n",
      "total_trades: 68623\n",
      "Sharpe: 0.950\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 168         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 146         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028953047 |\n",
      "|    clip_fraction        | 0.251       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.5       |\n",
      "|    explained_variance   | 0.00392     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 10.6        |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0268     |\n",
      "|    reward               | 0.8935994   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 25.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 168         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 158         |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022955451 |\n",
      "|    clip_fraction        | 0.201       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.5       |\n",
      "|    explained_variance   | 0.0313      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 5.96        |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.017      |\n",
      "|    reward               | 0.19411081  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 18.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 168         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 170         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023811536 |\n",
      "|    clip_fraction        | 0.231       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.5       |\n",
      "|    explained_variance   | 0.013       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 21.1        |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0197     |\n",
      "|    reward               | 0.09751167  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 30.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 168         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 182         |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017234607 |\n",
      "|    clip_fraction        | 0.183       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.6       |\n",
      "|    explained_variance   | 0.0143      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 14.2        |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0233     |\n",
      "|    reward               | 0.23108178  |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 26.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 168         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 194         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023931257 |\n",
      "|    clip_fraction        | 0.23        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.6       |\n",
      "|    explained_variance   | 0.00746     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 9           |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0236     |\n",
      "|    reward               | -0.2818211  |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 22.8        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 168        |\n",
      "|    iterations           | 17         |\n",
      "|    time_elapsed         | 206        |\n",
      "|    total_timesteps      | 34816      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02345245 |\n",
      "|    clip_fraction        | 0.257      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -41.6      |\n",
      "|    explained_variance   | 0.0256     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 7.47       |\n",
      "|    n_updates            | 160        |\n",
      "|    policy_gradient_loss | -0.0232    |\n",
      "|    reward               | -0.3748405 |\n",
      "|    std                  | 1.02       |\n",
      "|    value_loss           | 19.7       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 168         |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 218         |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.02683885  |\n",
      "|    clip_fraction        | 0.269       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.7       |\n",
      "|    explained_variance   | 0.00735     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 6.43        |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.015      |\n",
      "|    reward               | -0.18818314 |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 13.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 168         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 230         |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028695706 |\n",
      "|    clip_fraction        | 0.24        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.8       |\n",
      "|    explained_variance   | 0.0136      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 7.85        |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.019      |\n",
      "|    reward               | -1.6678649  |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 21.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 168         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 243         |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021477088 |\n",
      "|    clip_fraction        | 0.183       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.8       |\n",
      "|    explained_variance   | -0.0463     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 10.1        |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.02       |\n",
      "|    reward               | 0.37572563  |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 30.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 256         |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025265824 |\n",
      "|    clip_fraction        | 0.271       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.8       |\n",
      "|    explained_variance   | -0.0124     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 25.9        |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0163     |\n",
      "|    reward               | 0.38730854  |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 48.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 269         |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029471241 |\n",
      "|    clip_fraction        | 0.262       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.9       |\n",
      "|    explained_variance   | 0.0192      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 23.1        |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0188     |\n",
      "|    reward               | 0.28247014  |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 37.2        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 167        |\n",
      "|    iterations           | 23         |\n",
      "|    time_elapsed         | 281        |\n",
      "|    total_timesteps      | 47104      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04032046 |\n",
      "|    clip_fraction        | 0.319      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -41.9      |\n",
      "|    explained_variance   | -0.00101   |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 15.6       |\n",
      "|    n_updates            | 220        |\n",
      "|    policy_gradient_loss | -0.0147    |\n",
      "|    reward               | -1.7638819 |\n",
      "|    std                  | 1.03       |\n",
      "|    value_loss           | 38.8       |\n",
      "----------------------------------------\n",
      "day: 2514, episode: 40\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3904201.03\n",
      "total_reward: 2904201.03\n",
      "total_cost: 259706.88\n",
      "total_trades: 67256\n",
      "Sharpe: 0.918\n",
      "=================================\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 167        |\n",
      "|    iterations           | 24         |\n",
      "|    time_elapsed         | 293        |\n",
      "|    total_timesteps      | 49152      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03205637 |\n",
      "|    clip_fraction        | 0.315      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -41.9      |\n",
      "|    explained_variance   | 0.0136     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 7.73       |\n",
      "|    n_updates            | 230        |\n",
      "|    policy_gradient_loss | -0.0211    |\n",
      "|    reward               | 2.7354805  |\n",
      "|    std                  | 1.03       |\n",
      "|    value_loss           | 19.3       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 305         |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023974817 |\n",
      "|    clip_fraction        | 0.251       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42         |\n",
      "|    explained_variance   | 0.0173      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 16.2        |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0192     |\n",
      "|    reward               | 1.5140111   |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 41.9        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 167        |\n",
      "|    iterations           | 26         |\n",
      "|    time_elapsed         | 318        |\n",
      "|    total_timesteps      | 53248      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04027238 |\n",
      "|    clip_fraction        | 0.349      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -42        |\n",
      "|    explained_variance   | 0.00236    |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 5.59       |\n",
      "|    n_updates            | 250        |\n",
      "|    policy_gradient_loss | -0.0168    |\n",
      "|    reward               | -2.053411  |\n",
      "|    std                  | 1.03       |\n",
      "|    value_loss           | 15.2       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 330         |\n",
      "|    total_timesteps      | 55296       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036309533 |\n",
      "|    clip_fraction        | 0.325       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.1       |\n",
      "|    explained_variance   | -0.00966    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 5.2         |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0164     |\n",
      "|    reward               | -0.8681482  |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 14.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 342         |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030443171 |\n",
      "|    clip_fraction        | 0.283       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.2       |\n",
      "|    explained_variance   | 0.0117      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 5.67        |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.0149     |\n",
      "|    reward               | 1.2931861   |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 15.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 354         |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033963766 |\n",
      "|    clip_fraction        | 0.288       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.2       |\n",
      "|    explained_variance   | 0.0764      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 5.54        |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0164     |\n",
      "|    reward               | -1.1633044  |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 12.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 366         |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036064804 |\n",
      "|    clip_fraction        | 0.332       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.3       |\n",
      "|    explained_variance   | 0.0373      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 7.47        |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.00846    |\n",
      "|    reward               | 0.3815144   |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 18.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 378         |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021008529 |\n",
      "|    clip_fraction        | 0.234       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.3       |\n",
      "|    explained_variance   | 0.0404      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 17.7        |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0138     |\n",
      "|    reward               | 0.49551892  |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 22.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 390         |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.01987023  |\n",
      "|    clip_fraction        | 0.2         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.3       |\n",
      "|    explained_variance   | 0.0893      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 6.42        |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.0159     |\n",
      "|    reward               | -0.16073525 |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 20          |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 167        |\n",
      "|    iterations           | 33         |\n",
      "|    time_elapsed         | 402        |\n",
      "|    total_timesteps      | 67584      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02953197 |\n",
      "|    clip_fraction        | 0.261      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -42.4      |\n",
      "|    explained_variance   | 0.0585     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 8.84       |\n",
      "|    n_updates            | 320        |\n",
      "|    policy_gradient_loss | -0.00988   |\n",
      "|    reward               | 4.2507977  |\n",
      "|    std                  | 1.04       |\n",
      "|    value_loss           | 21.8       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 414         |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029604178 |\n",
      "|    clip_fraction        | 0.284       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.4       |\n",
      "|    explained_variance   | 0.0599      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 9.8         |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.0145     |\n",
      "|    reward               | 2.70608     |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 20.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 168         |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 426         |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022597037 |\n",
      "|    clip_fraction        | 0.253       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.4       |\n",
      "|    explained_variance   | 0.0861      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 13.5        |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.018      |\n",
      "|    reward               | -2.3449075  |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 19.4        |\n",
      "-----------------------------------------\n",
      "day: 2514, episode: 50\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2964150.12\n",
      "total_reward: 1964150.12\n",
      "total_cost: 221584.91\n",
      "total_trades: 64016\n",
      "Sharpe: 0.796\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 168         |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 438         |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042792775 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.5       |\n",
      "|    explained_variance   | 0.15        |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 11.2        |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.00513    |\n",
      "|    reward               | -0.14303435 |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 22.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 168         |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 450         |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.0434477   |\n",
      "|    clip_fraction        | 0.306       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.5       |\n",
      "|    explained_variance   | 0.0317      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 14.5        |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | 0.00379     |\n",
      "|    reward               | 0.050019752 |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 28.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 168         |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 462         |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023645068 |\n",
      "|    clip_fraction        | 0.25        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.5       |\n",
      "|    explained_variance   | 0.124       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 10.8        |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.0126     |\n",
      "|    reward               | 1.0780873   |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 20.1        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 168        |\n",
      "|    iterations           | 39         |\n",
      "|    time_elapsed         | 474        |\n",
      "|    total_timesteps      | 79872      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01592827 |\n",
      "|    clip_fraction        | 0.156      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -42.5      |\n",
      "|    explained_variance   | 0.206      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 12.1       |\n",
      "|    n_updates            | 380        |\n",
      "|    policy_gradient_loss | -0.0102    |\n",
      "|    reward               | 0.35945332 |\n",
      "|    std                  | 1.05       |\n",
      "|    value_loss           | 22.2       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 168         |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 486         |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027171226 |\n",
      "|    clip_fraction        | 0.262       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.6       |\n",
      "|    explained_variance   | 0.177       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 5.96        |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.0172     |\n",
      "|    reward               | -0.05918642 |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 12.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 168         |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 497         |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028834278 |\n",
      "|    clip_fraction        | 0.307       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.6       |\n",
      "|    explained_variance   | 0.131       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 7.45        |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.0101     |\n",
      "|    reward               | 1.0457264   |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 13.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 168         |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 509         |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031922672 |\n",
      "|    clip_fraction        | 0.279       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.7       |\n",
      "|    explained_variance   | 0.179       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 7.09        |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.0134     |\n",
      "|    reward               | 0.05547702  |\n",
      "|    std                  | 1.06        |\n",
      "|    value_loss           | 16.1        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 168        |\n",
      "|    iterations           | 43         |\n",
      "|    time_elapsed         | 521        |\n",
      "|    total_timesteps      | 88064      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03118943 |\n",
      "|    clip_fraction        | 0.245      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -42.7      |\n",
      "|    explained_variance   | 0.0849     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 12.5       |\n",
      "|    n_updates            | 420        |\n",
      "|    policy_gradient_loss | -0.0129    |\n",
      "|    reward               | 0.155471   |\n",
      "|    std                  | 1.06       |\n",
      "|    value_loss           | 29.6       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 168         |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 533         |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037384547 |\n",
      "|    clip_fraction        | 0.343       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.8       |\n",
      "|    explained_variance   | 0.0306      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 16.2        |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.00816    |\n",
      "|    reward               | -2.1969707  |\n",
      "|    std                  | 1.06        |\n",
      "|    value_loss           | 33.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 168         |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 545         |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022786967 |\n",
      "|    clip_fraction        | 0.201       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.9       |\n",
      "|    explained_variance   | 0.171       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 18.2        |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.012      |\n",
      "|    reward               | 0.06050079  |\n",
      "|    std                  | 1.06        |\n",
      "|    value_loss           | 25.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 168         |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 557         |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027277222 |\n",
      "|    clip_fraction        | 0.227       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.9       |\n",
      "|    explained_variance   | 0.166       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 25.6        |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.0118     |\n",
      "|    reward               | 2.320119    |\n",
      "|    std                  | 1.07        |\n",
      "|    value_loss           | 33.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 168         |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 569         |\n",
      "|    total_timesteps      | 96256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.01946489  |\n",
      "|    clip_fraction        | 0.178       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43         |\n",
      "|    explained_variance   | 0.109       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 19.7        |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0102     |\n",
      "|    reward               | -0.55221856 |\n",
      "|    std                  | 1.07        |\n",
      "|    value_loss           | 33.7        |\n",
      "-----------------------------------------\n",
      "day: 2514, episode: 60\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5054156.41\n",
      "total_reward: 4054156.41\n",
      "total_cost: 223622.13\n",
      "total_trades: 64087\n",
      "Sharpe: 1.102\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 581         |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021290679 |\n",
      "|    clip_fraction        | 0.214       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43         |\n",
      "|    explained_variance   | 0.118       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 19          |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.0026     |\n",
      "|    reward               | -0.17624289 |\n",
      "|    std                  | 1.07        |\n",
      "|    value_loss           | 36.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 49          |\n",
      "|    time_elapsed         | 593         |\n",
      "|    total_timesteps      | 100352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033710346 |\n",
      "|    clip_fraction        | 0.327       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43         |\n",
      "|    explained_variance   | 0.0732      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 14.4        |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.00657    |\n",
      "|    reward               | 3.3942149   |\n",
      "|    std                  | 1.07        |\n",
      "|    value_loss           | 36.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 605         |\n",
      "|    total_timesteps      | 102400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038263723 |\n",
      "|    clip_fraction        | 0.405       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.1       |\n",
      "|    explained_variance   | 0.087       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 12.3        |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | 0.00247     |\n",
      "|    reward               | -0.35382572 |\n",
      "|    std                  | 1.07        |\n",
      "|    value_loss           | 32.5        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 169        |\n",
      "|    iterations           | 51         |\n",
      "|    time_elapsed         | 617        |\n",
      "|    total_timesteps      | 104448     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03007848 |\n",
      "|    clip_fraction        | 0.325      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -43.1      |\n",
      "|    explained_variance   | 0.211      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 10.8       |\n",
      "|    n_updates            | 500        |\n",
      "|    policy_gradient_loss | -0.013     |\n",
      "|    reward               | 1.7333199  |\n",
      "|    std                  | 1.07       |\n",
      "|    value_loss           | 24.6       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 629         |\n",
      "|    total_timesteps      | 106496      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033195518 |\n",
      "|    clip_fraction        | 0.292       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.2       |\n",
      "|    explained_variance   | 0.124       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 15.7        |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.0124     |\n",
      "|    reward               | 2.7040956   |\n",
      "|    std                  | 1.07        |\n",
      "|    value_loss           | 37.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 641         |\n",
      "|    total_timesteps      | 108544      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017363366 |\n",
      "|    clip_fraction        | 0.175       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.2       |\n",
      "|    explained_variance   | 0.0981      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 20.7        |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0114     |\n",
      "|    reward               | 0.56045866  |\n",
      "|    std                  | 1.08        |\n",
      "|    value_loss           | 38.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 54          |\n",
      "|    time_elapsed         | 653         |\n",
      "|    total_timesteps      | 110592      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035761666 |\n",
      "|    clip_fraction        | 0.274       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.3       |\n",
      "|    explained_variance   | 0.192       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 27.5        |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.00351    |\n",
      "|    reward               | -2.1835825  |\n",
      "|    std                  | 1.08        |\n",
      "|    value_loss           | 41.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 55          |\n",
      "|    time_elapsed         | 665         |\n",
      "|    total_timesteps      | 112640      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031289265 |\n",
      "|    clip_fraction        | 0.292       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.3       |\n",
      "|    explained_variance   | 0.0772      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 24.2        |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.00846    |\n",
      "|    reward               | -0.2537541  |\n",
      "|    std                  | 1.08        |\n",
      "|    value_loss           | 44.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 56          |\n",
      "|    time_elapsed         | 676         |\n",
      "|    total_timesteps      | 114688      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040413737 |\n",
      "|    clip_fraction        | 0.286       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.4       |\n",
      "|    explained_variance   | 0.349       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 11.1        |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | -0.0103     |\n",
      "|    reward               | -5.9544353  |\n",
      "|    std                  | 1.08        |\n",
      "|    value_loss           | 27.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 57          |\n",
      "|    time_elapsed         | 688         |\n",
      "|    total_timesteps      | 116736      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026474461 |\n",
      "|    clip_fraction        | 0.209       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.4       |\n",
      "|    explained_variance   | 0.292       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 15.3        |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0126     |\n",
      "|    reward               | -1.6946508  |\n",
      "|    std                  | 1.08        |\n",
      "|    value_loss           | 34.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 58          |\n",
      "|    time_elapsed         | 700         |\n",
      "|    total_timesteps      | 118784      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021372948 |\n",
      "|    clip_fraction        | 0.186       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.5       |\n",
      "|    explained_variance   | 0.304       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 16.2        |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.00649    |\n",
      "|    reward               | 1.7258941   |\n",
      "|    std                  | 1.08        |\n",
      "|    value_loss           | 35.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 59          |\n",
      "|    time_elapsed         | 712         |\n",
      "|    total_timesteps      | 120832      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026570667 |\n",
      "|    clip_fraction        | 0.26        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.5       |\n",
      "|    explained_variance   | 0.262       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 32.1        |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.00647    |\n",
      "|    reward               | 2.2037783   |\n",
      "|    std                  | 1.09        |\n",
      "|    value_loss           | 59.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 60          |\n",
      "|    time_elapsed         | 724         |\n",
      "|    total_timesteps      | 122880      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023274692 |\n",
      "|    clip_fraction        | 0.279       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.6       |\n",
      "|    explained_variance   | 0.12        |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 18.5        |\n",
      "|    n_updates            | 590         |\n",
      "|    policy_gradient_loss | -0.0095     |\n",
      "|    reward               | 0.9131043   |\n",
      "|    std                  | 1.09        |\n",
      "|    value_loss           | 37.9        |\n",
      "-----------------------------------------\n",
      "day: 2514, episode: 70\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4255505.53\n",
      "total_reward: 3255505.53\n",
      "total_cost: 186835.72\n",
      "total_trades: 61201\n",
      "Sharpe: 0.911\n",
      "=================================\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 169        |\n",
      "|    iterations           | 61         |\n",
      "|    time_elapsed         | 736        |\n",
      "|    total_timesteps      | 124928     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02293485 |\n",
      "|    clip_fraction        | 0.232      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -43.6      |\n",
      "|    explained_variance   | 0.118      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 15.3       |\n",
      "|    n_updates            | 600        |\n",
      "|    policy_gradient_loss | -0.0121    |\n",
      "|    reward               | -2.925555  |\n",
      "|    std                  | 1.09       |\n",
      "|    value_loss           | 30.6       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 62          |\n",
      "|    time_elapsed         | 748         |\n",
      "|    total_timesteps      | 126976      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026606128 |\n",
      "|    clip_fraction        | 0.219       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.7       |\n",
      "|    explained_variance   | 0.266       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 8.02        |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | -0.0146     |\n",
      "|    reward               | -3.6289775  |\n",
      "|    std                  | 1.09        |\n",
      "|    value_loss           | 31.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 63          |\n",
      "|    time_elapsed         | 760         |\n",
      "|    total_timesteps      | 129024      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.061073832 |\n",
      "|    clip_fraction        | 0.301       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.7       |\n",
      "|    explained_variance   | 0.404       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 17.3        |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.0033     |\n",
      "|    reward               | 0.7790242   |\n",
      "|    std                  | 1.09        |\n",
      "|    value_loss           | 39.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 64          |\n",
      "|    time_elapsed         | 772         |\n",
      "|    total_timesteps      | 131072      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029746111 |\n",
      "|    clip_fraction        | 0.295       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.8       |\n",
      "|    explained_variance   | 0.217       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 36.2        |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | -0.00247    |\n",
      "|    reward               | -0.14046648 |\n",
      "|    std                  | 1.1         |\n",
      "|    value_loss           | 67.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 65          |\n",
      "|    time_elapsed         | 784         |\n",
      "|    total_timesteps      | 133120      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028684143 |\n",
      "|    clip_fraction        | 0.24        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.9       |\n",
      "|    explained_variance   | 0.242       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 14.3        |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.00449    |\n",
      "|    reward               | 5.4517426   |\n",
      "|    std                  | 1.1         |\n",
      "|    value_loss           | 29.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 66          |\n",
      "|    time_elapsed         | 796         |\n",
      "|    total_timesteps      | 135168      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.043044012 |\n",
      "|    clip_fraction        | 0.281       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44         |\n",
      "|    explained_variance   | 0.0833      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 28.5        |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.00498    |\n",
      "|    reward               | 0.22333941  |\n",
      "|    std                  | 1.1         |\n",
      "|    value_loss           | 45          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 67          |\n",
      "|    time_elapsed         | 808         |\n",
      "|    total_timesteps      | 137216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019876825 |\n",
      "|    clip_fraction        | 0.248       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44         |\n",
      "|    explained_variance   | 0.281       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 20          |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.00724    |\n",
      "|    reward               | 0.9399907   |\n",
      "|    std                  | 1.11        |\n",
      "|    value_loss           | 43.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 68          |\n",
      "|    time_elapsed         | 820         |\n",
      "|    total_timesteps      | 139264      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029118408 |\n",
      "|    clip_fraction        | 0.309       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.1       |\n",
      "|    explained_variance   | 0.387       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 15.9        |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | -0.00578    |\n",
      "|    reward               | 0.91921103  |\n",
      "|    std                  | 1.11        |\n",
      "|    value_loss           | 33.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 69          |\n",
      "|    time_elapsed         | 833         |\n",
      "|    total_timesteps      | 141312      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020518377 |\n",
      "|    clip_fraction        | 0.246       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.1       |\n",
      "|    explained_variance   | 0.316       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 15.2        |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.00088    |\n",
      "|    reward               | 0.17761362  |\n",
      "|    std                  | 1.11        |\n",
      "|    value_loss           | 32.2        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 169          |\n",
      "|    iterations           | 70           |\n",
      "|    time_elapsed         | 845          |\n",
      "|    total_timesteps      | 143360       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.032132894  |\n",
      "|    clip_fraction        | 0.329        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -44.1        |\n",
      "|    explained_variance   | 0.286        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 35.8         |\n",
      "|    n_updates            | 690          |\n",
      "|    policy_gradient_loss | 0.00361      |\n",
      "|    reward               | 0.0032663038 |\n",
      "|    std                  | 1.11         |\n",
      "|    value_loss           | 80.9         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 71          |\n",
      "|    time_elapsed         | 857         |\n",
      "|    total_timesteps      | 145408      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019071901 |\n",
      "|    clip_fraction        | 0.179       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.1       |\n",
      "|    explained_variance   | 0.0864      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 27.7        |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.0074     |\n",
      "|    reward               | -2.4843266  |\n",
      "|    std                  | 1.11        |\n",
      "|    value_loss           | 55.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 72          |\n",
      "|    time_elapsed         | 869         |\n",
      "|    total_timesteps      | 147456      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024507508 |\n",
      "|    clip_fraction        | 0.297       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.2       |\n",
      "|    explained_variance   | 0.361       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 12.2        |\n",
      "|    n_updates            | 710         |\n",
      "|    policy_gradient_loss | -0.0111     |\n",
      "|    reward               | -1.9815828  |\n",
      "|    std                  | 1.11        |\n",
      "|    value_loss           | 40.2        |\n",
      "-----------------------------------------\n",
      "day: 2514, episode: 80\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4279025.29\n",
      "total_reward: 3279025.29\n",
      "total_cost: 129679.12\n",
      "total_trades: 56589\n",
      "Sharpe: 0.826\n",
      "=================================\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 169        |\n",
      "|    iterations           | 73         |\n",
      "|    time_elapsed         | 881        |\n",
      "|    total_timesteps      | 149504     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01598022 |\n",
      "|    clip_fraction        | 0.194      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -44.2      |\n",
      "|    explained_variance   | 0.292      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 29.6       |\n",
      "|    n_updates            | 720        |\n",
      "|    policy_gradient_loss | -0.00311   |\n",
      "|    reward               | 0.20319223 |\n",
      "|    std                  | 1.11       |\n",
      "|    value_loss           | 50.9       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 74          |\n",
      "|    time_elapsed         | 893         |\n",
      "|    total_timesteps      | 151552      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018214207 |\n",
      "|    clip_fraction        | 0.119       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.2       |\n",
      "|    explained_variance   | 0.381       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 17.9        |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | -0.00869    |\n",
      "|    reward               | 1.9113276   |\n",
      "|    std                  | 1.11        |\n",
      "|    value_loss           | 49.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 75          |\n",
      "|    time_elapsed         | 905         |\n",
      "|    total_timesteps      | 153600      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.049670048 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.3       |\n",
      "|    explained_variance   | 0.349       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 36.7        |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | 0.00565     |\n",
      "|    reward               | 0.21977682  |\n",
      "|    std                  | 1.12        |\n",
      "|    value_loss           | 57.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 76          |\n",
      "|    time_elapsed         | 917         |\n",
      "|    total_timesteps      | 155648      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036422893 |\n",
      "|    clip_fraction        | 0.32        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.4       |\n",
      "|    explained_variance   | 0.3         |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 24.7        |\n",
      "|    n_updates            | 750         |\n",
      "|    policy_gradient_loss | 0.00594     |\n",
      "|    reward               | -2.2857342  |\n",
      "|    std                  | 1.12        |\n",
      "|    value_loss           | 55.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 77          |\n",
      "|    time_elapsed         | 929         |\n",
      "|    total_timesteps      | 157696      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024643732 |\n",
      "|    clip_fraction        | 0.324       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.4       |\n",
      "|    explained_variance   | 0.27        |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 20.3        |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | 0.00152     |\n",
      "|    reward               | 1.8686676   |\n",
      "|    std                  | 1.12        |\n",
      "|    value_loss           | 40.9        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 169        |\n",
      "|    iterations           | 78         |\n",
      "|    time_elapsed         | 941        |\n",
      "|    total_timesteps      | 159744     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04615508 |\n",
      "|    clip_fraction        | 0.333      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -44.5      |\n",
      "|    explained_variance   | 0.343      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 21.7       |\n",
      "|    n_updates            | 770        |\n",
      "|    policy_gradient_loss | -0.00279   |\n",
      "|    reward               | 0.17715366 |\n",
      "|    std                  | 1.12       |\n",
      "|    value_loss           | 45.4       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 79          |\n",
      "|    time_elapsed         | 953         |\n",
      "|    total_timesteps      | 161792      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028554287 |\n",
      "|    clip_fraction        | 0.254       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.5       |\n",
      "|    explained_variance   | 0.337       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 25.9        |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.00123    |\n",
      "|    reward               | 0.08795386  |\n",
      "|    std                  | 1.12        |\n",
      "|    value_loss           | 48.1        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 169        |\n",
      "|    iterations           | 80         |\n",
      "|    time_elapsed         | 965        |\n",
      "|    total_timesteps      | 163840     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03527516 |\n",
      "|    clip_fraction        | 0.276      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -44.6      |\n",
      "|    explained_variance   | 0.299      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 43.9       |\n",
      "|    n_updates            | 790        |\n",
      "|    policy_gradient_loss | 0.0114     |\n",
      "|    reward               | -1.2276275 |\n",
      "|    std                  | 1.13       |\n",
      "|    value_loss           | 70.7       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 81          |\n",
      "|    time_elapsed         | 977         |\n",
      "|    total_timesteps      | 165888      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029780485 |\n",
      "|    clip_fraction        | 0.258       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.6       |\n",
      "|    explained_variance   | 0.269       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 14.4        |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | 0.00291     |\n",
      "|    reward               | 8.159967    |\n",
      "|    std                  | 1.13        |\n",
      "|    value_loss           | 30.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 82          |\n",
      "|    time_elapsed         | 989         |\n",
      "|    total_timesteps      | 167936      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021451257 |\n",
      "|    clip_fraction        | 0.186       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.6       |\n",
      "|    explained_variance   | 0.0691      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 32.2        |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.00358    |\n",
      "|    reward               | -1.0774804  |\n",
      "|    std                  | 1.13        |\n",
      "|    value_loss           | 57.9        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 169        |\n",
      "|    iterations           | 83         |\n",
      "|    time_elapsed         | 1002       |\n",
      "|    total_timesteps      | 169984     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02793467 |\n",
      "|    clip_fraction        | 0.324      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -44.6      |\n",
      "|    explained_variance   | 0.312      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 35.2       |\n",
      "|    n_updates            | 820        |\n",
      "|    policy_gradient_loss | -0.000807  |\n",
      "|    reward               | 3.582471   |\n",
      "|    std                  | 1.13       |\n",
      "|    value_loss           | 59.8       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 84          |\n",
      "|    time_elapsed         | 1015        |\n",
      "|    total_timesteps      | 172032      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030117895 |\n",
      "|    clip_fraction        | 0.26        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.7       |\n",
      "|    explained_variance   | 0.341       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 33.9        |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | 0.00047     |\n",
      "|    reward               | -1.1345054  |\n",
      "|    std                  | 1.13        |\n",
      "|    value_loss           | 62.6        |\n",
      "-----------------------------------------\n",
      "day: 2514, episode: 90\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5781680.25\n",
      "total_reward: 4781680.25\n",
      "total_cost: 111647.64\n",
      "total_trades: 55934\n",
      "Sharpe: 1.021\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 85          |\n",
      "|    time_elapsed         | 1027        |\n",
      "|    total_timesteps      | 174080      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.03455466  |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.8       |\n",
      "|    explained_variance   | 0.364       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 35.7        |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | 0.00428     |\n",
      "|    reward               | -0.43864045 |\n",
      "|    std                  | 1.13        |\n",
      "|    value_loss           | 55.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 86          |\n",
      "|    time_elapsed         | 1039        |\n",
      "|    total_timesteps      | 176128      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022060659 |\n",
      "|    clip_fraction        | 0.332       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.8       |\n",
      "|    explained_variance   | 0.351       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 36.4        |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | 0.00524     |\n",
      "|    reward               | -0.80059206 |\n",
      "|    std                  | 1.14        |\n",
      "|    value_loss           | 83.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 87          |\n",
      "|    time_elapsed         | 1051        |\n",
      "|    total_timesteps      | 178176      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023012077 |\n",
      "|    clip_fraction        | 0.266       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.9       |\n",
      "|    explained_variance   | 0.208       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 29.4        |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | 0.00331     |\n",
      "|    reward               | 3.2407825   |\n",
      "|    std                  | 1.14        |\n",
      "|    value_loss           | 65.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 88          |\n",
      "|    time_elapsed         | 1063        |\n",
      "|    total_timesteps      | 180224      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024956355 |\n",
      "|    clip_fraction        | 0.232       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.9       |\n",
      "|    explained_variance   | 0.144       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 13.1        |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | -0.00186    |\n",
      "|    reward               | 2.0607448   |\n",
      "|    std                  | 1.14        |\n",
      "|    value_loss           | 27.8        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 169        |\n",
      "|    iterations           | 89         |\n",
      "|    time_elapsed         | 1075       |\n",
      "|    total_timesteps      | 182272     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03057125 |\n",
      "|    clip_fraction        | 0.244      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -44.9      |\n",
      "|    explained_variance   | 0.359      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 17.7       |\n",
      "|    n_updates            | 880        |\n",
      "|    policy_gradient_loss | -0.00956   |\n",
      "|    reward               | -1.4391896 |\n",
      "|    std                  | 1.14       |\n",
      "|    value_loss           | 45.1       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 90          |\n",
      "|    time_elapsed         | 1087        |\n",
      "|    total_timesteps      | 184320      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034999408 |\n",
      "|    clip_fraction        | 0.303       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.9       |\n",
      "|    explained_variance   | 0.376       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 26.5        |\n",
      "|    n_updates            | 890         |\n",
      "|    policy_gradient_loss | 0.00357     |\n",
      "|    reward               | 1.9186112   |\n",
      "|    std                  | 1.14        |\n",
      "|    value_loss           | 53.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 91          |\n",
      "|    time_elapsed         | 1100        |\n",
      "|    total_timesteps      | 186368      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021063253 |\n",
      "|    clip_fraction        | 0.188       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45         |\n",
      "|    explained_variance   | 0.4         |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 31.6        |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.00334    |\n",
      "|    reward               | 0.22587165  |\n",
      "|    std                  | 1.14        |\n",
      "|    value_loss           | 67.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 92          |\n",
      "|    time_elapsed         | 1113        |\n",
      "|    total_timesteps      | 188416      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031479914 |\n",
      "|    clip_fraction        | 0.327       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45         |\n",
      "|    explained_variance   | 0.405       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 27.1        |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | 0.00407     |\n",
      "|    reward               | -0.75166047 |\n",
      "|    std                  | 1.14        |\n",
      "|    value_loss           | 56.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 168         |\n",
      "|    iterations           | 93          |\n",
      "|    time_elapsed         | 1127        |\n",
      "|    total_timesteps      | 190464      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018326676 |\n",
      "|    clip_fraction        | 0.193       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45         |\n",
      "|    explained_variance   | 0.402       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 17.5        |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.00393    |\n",
      "|    reward               | 2.412545    |\n",
      "|    std                  | 1.14        |\n",
      "|    value_loss           | 30.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 168         |\n",
      "|    iterations           | 94          |\n",
      "|    time_elapsed         | 1142        |\n",
      "|    total_timesteps      | 192512      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018855056 |\n",
      "|    clip_fraction        | 0.163       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45         |\n",
      "|    explained_variance   | 0.404       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 20.4        |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | -0.0034     |\n",
      "|    reward               | -0.59090793 |\n",
      "|    std                  | 1.14        |\n",
      "|    value_loss           | 41.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 168         |\n",
      "|    iterations           | 95          |\n",
      "|    time_elapsed         | 1155        |\n",
      "|    total_timesteps      | 194560      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017812852 |\n",
      "|    clip_fraction        | 0.186       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.1       |\n",
      "|    explained_variance   | 0.4         |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 19.4        |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.00485    |\n",
      "|    reward               | -1.3102553  |\n",
      "|    std                  | 1.15        |\n",
      "|    value_loss           | 43.4        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 168        |\n",
      "|    iterations           | 96         |\n",
      "|    time_elapsed         | 1168       |\n",
      "|    total_timesteps      | 196608     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03379704 |\n",
      "|    clip_fraction        | 0.331      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -45.1      |\n",
      "|    explained_variance   | 0.48       |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 23.2       |\n",
      "|    n_updates            | 950        |\n",
      "|    policy_gradient_loss | -0.00357   |\n",
      "|    reward               | -2.1187892 |\n",
      "|    std                  | 1.15       |\n",
      "|    value_loss           | 51.4       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 168         |\n",
      "|    iterations           | 97          |\n",
      "|    time_elapsed         | 1181        |\n",
      "|    total_timesteps      | 198656      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024312943 |\n",
      "|    clip_fraction        | 0.263       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.2       |\n",
      "|    explained_variance   | 0.359       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 19          |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | 0.00373     |\n",
      "|    reward               | -0.29158872 |\n",
      "|    std                  | 1.15        |\n",
      "|    value_loss           | 49.4        |\n",
      "-----------------------------------------\n",
      "day: 2514, episode: 100\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5559212.27\n",
      "total_reward: 4559212.27\n",
      "total_cost: 159238.52\n",
      "total_trades: 58975\n",
      "Sharpe: 1.041\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 98          |\n",
      "|    time_elapsed         | 1194        |\n",
      "|    total_timesteps      | 200704      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011730239 |\n",
      "|    clip_fraction        | 0.175       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.2       |\n",
      "|    explained_variance   | 0.0417      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 24.9        |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | -0.00195    |\n",
      "|    reward               | -0.28323546 |\n",
      "|    std                  | 1.15        |\n",
      "|    value_loss           | 68          |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "trained_ppo = agent.train_model(model=model_ppo,\n",
    "                             tb_log_name='ppo',\n",
    "                             total_timesteps=200000) if if_using_ppo else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1730813059610,
     "user": {
      "displayName": "Natty Metekie",
      "userId": "06571578933818431366"
     },
     "user_tz": -240
    },
    "id": "cGNmES1ifp1O",
    "outputId": "e975c5dc-8d57-4e2c-88a6-55fd8044562a"
   },
   "outputs": [],
   "source": [
    "trained_ppo.save(config.TRAINED_MODEL_DIR + \"/agent_ppo\") if if_using_ppo else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YOFOGFCPqmot"
   },
   "source": [
    "Trading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "DYpOmJMpqmou"
   },
   "outputs": [],
   "source": [
    "e_trade_gym = StockTradingEnv(df = trade, **env_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 513,
     "status": "ok",
     "timestamp": 1730813060118,
     "user": {
      "displayName": "Natty Metekie",
      "userId": "06571578933818431366"
     },
     "user_tz": -240
    },
    "id": "kA3U0qJEqmov",
    "outputId": "f0d7437f-8c26-4bbc-e68c-3fdb974ca3a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hit end!\n"
     ]
    }
   ],
   "source": [
    "df_account_value, df_actions = DRLAgent.DRL_prediction(\n",
    "    model=trained_ppo,\n",
    "    environment = e_trade_gym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 261
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1730813060119,
     "user": {
      "displayName": "Natty Metekie",
      "userId": "06571578933818431366"
     },
     "user_tz": -240
    },
    "id": "nI1625Feqmov",
    "outputId": "d3bbbeed-ceea-46bd-e074-191b84c4b4c8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>account_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>2020-09-23</td>\n",
       "      <td>900814.247640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>2020-09-24</td>\n",
       "      <td>906593.959866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>2020-09-25</td>\n",
       "      <td>919445.367209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>2020-09-28</td>\n",
       "      <td>932056.240344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>2020-09-29</td>\n",
       "      <td>927069.374144</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  account_value\n",
       "183  2020-09-23  900814.247640\n",
       "184  2020-09-24  906593.959866\n",
       "185  2020-09-25  919445.367209\n",
       "186  2020-09-28  932056.240344\n",
       "187  2020-09-29  927069.374144"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_account_value.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VEcli8bgqmow"
   },
   "source": [
    "Backtesting Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1730813060119,
     "user": {
      "displayName": "Natty Metekie",
      "userId": "06571578933818431366"
     },
     "user_tz": -240
    },
    "id": "rKUbZkdjqmow",
    "outputId": "ea521dd0-7f0b-48f5-aa7a-b690454a091c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============Get Backtest Results===========\n",
      "Annual return         -0.096524\n",
      "Cumulative returns    -0.072931\n",
      "Annual volatility      0.426166\n",
      "Sharpe ratio          -0.026455\n",
      "Calmar ratio          -0.276040\n",
      "Stability              0.005532\n",
      "Max drawdown          -0.349675\n",
      "Omega ratio            0.994663\n",
      "Sortino ratio         -0.036719\n",
      "Skew                        NaN\n",
      "Kurtosis                    NaN\n",
      "Tail ratio             0.857310\n",
      "Daily value at risk   -0.053737\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"==============Get Backtest Results===========\")\n",
    "now = datetime.datetime.now().strftime('%Y%m%d-%Hh%M')\n",
    "\n",
    "perf_stats_all = backtest_stats(account_value=df_account_value)\n",
    "perf_stats_all = pd.DataFrame(perf_stats_all)\n",
    "perf_stats_all.to_csv(\"./\"+config.RESULTS_DIR+\"/perf_stats_all_\"+now+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1730813060119,
     "user": {
      "displayName": "Natty Metekie",
      "userId": "06571578933818431366"
     },
     "user_tz": -240
    },
    "id": "gVR-Z6mqqmox",
    "outputId": "286d4e4e-80e4-44b4-aab3-58b31eaace4a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============Get Baseline Stats===========\n",
      "Shape of DataFrame:  (188, 8)\n",
      "Annual return         -0.065199\n",
      "Cumulative returns    -0.049054\n",
      "Annual volatility      0.416030\n",
      "Sharpe ratio           0.046016\n",
      "Calmar ratio          -0.175803\n",
      "Stability              0.012240\n",
      "Max drawdown          -0.370862\n",
      "Omega ratio            1.009343\n",
      "Sortino ratio          0.062829\n",
      "Skew                        NaN\n",
      "Kurtosis                    NaN\n",
      "Tail ratio             0.860019\n",
      "Daily value at risk   -0.052339\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#baseline stats\n",
    "print(\"==============Get Baseline Stats===========\")\n",
    "baseline_df = get_baseline(\n",
    "        ticker=\"^DJI\",\n",
    "        start = '2020-01-01',\n",
    "        end = '2020-09-30')\n",
    "\n",
    "stats = backtest_stats(baseline_df, value_col_name = 'close')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 509
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1730813060119,
     "user": {
      "displayName": "Natty Metekie",
      "userId": "06571578933818431366"
     },
     "user_tz": -240
    },
    "id": "xTYDqDq-qmox",
    "outputId": "050c18d4-417e-47f1-cd39-685604914641"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ppo</th>\n",
       "      <th>dji</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-01-02</th>\n",
       "      <td>1.000000e+06</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-03</th>\n",
       "      <td>9.993839e+05</td>\n",
       "      <td>1.000000e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-06</th>\n",
       "      <td>9.997275e+05</td>\n",
       "      <td>1.002392e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-07</th>\n",
       "      <td>9.990258e+05</td>\n",
       "      <td>9.982119e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-08</th>\n",
       "      <td>1.000956e+06</td>\n",
       "      <td>1.003848e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-23</th>\n",
       "      <td>9.008142e+05</td>\n",
       "      <td>9.346322e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-24</th>\n",
       "      <td>9.065940e+05</td>\n",
       "      <td>9.364587e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-25</th>\n",
       "      <td>9.194454e+05</td>\n",
       "      <td>9.489818e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-28</th>\n",
       "      <td>9.320562e+05</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-29</th>\n",
       "      <td>9.270694e+05</td>\n",
       "      <td>9.587147e+05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>188 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     ppo           dji\n",
       "date                                  \n",
       "2020-01-02  1.000000e+06           NaN\n",
       "2020-01-03  9.993839e+05  1.000000e+06\n",
       "2020-01-06  9.997275e+05  1.002392e+06\n",
       "2020-01-07  9.990258e+05  9.982119e+05\n",
       "2020-01-08  1.000956e+06  1.003848e+06\n",
       "...                  ...           ...\n",
       "2020-09-23  9.008142e+05  9.346322e+05\n",
       "2020-09-24  9.065940e+05  9.364587e+05\n",
       "2020-09-25  9.194454e+05  9.489818e+05\n",
       "2020-09-28  9.320562e+05           NaN\n",
       "2020-09-29  9.270694e+05  9.587147e+05\n",
       "\n",
       "[188 rows x 2 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result_ppo = df_account_value.set_index(df_account_value.columns[0])\n",
    "result = pd.DataFrame(\n",
    "    {\n",
    "        \"ppo\": df_result_ppo[\"account_value\"],\n",
    "        \"dji\": dji[\"close\"],\n",
    "    }\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1730813060119,
     "user": {
      "displayName": "Natty Metekie",
      "userId": "06571578933818431366"
     },
     "user_tz": -240
    },
    "id": "zn0xNZUDqmox",
    "outputId": "c18e01a6-242f-4ab9-df16-7e7ef9752e64"
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (15,5)\n",
    "plt.figure()\n",
    "result.plot()\n",
    "\n",
    "plt.savefig('results_ppo.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JQetK8uJfuFs"
   },
   "source": [
    "A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1730813060119,
     "user": {
      "displayName": "Natty Metekie",
      "userId": "06571578933818431366"
     },
     "user_tz": -240
    },
    "id": "qAF3LFt3fwlV",
    "outputId": "8ff8f141-b8d8-4739-a97b-84fa9bd19f28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_steps': 5, 'ent_coef': 0.01, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to results/a2c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aboba/anaconda3/envs/finrl_env/lib/python3.11/site-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run A2C on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "model_a2c = agent.get_model(\"a2c\")\n",
    "\n",
    "if if_using_a2c:\n",
    "  # set up logger\n",
    "  tmp_path = config.RESULTS_DIR + '/a2c'\n",
    "  new_logger_a2c = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_a2c.set_logger(new_logger_a2c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 568954,
     "status": "ok",
     "timestamp": 1730813629062,
     "user": {
      "displayName": "Natty Metekie",
      "userId": "06571578933818431366"
     },
     "user_tz": -240
    },
    "id": "styOb2lIf3Wu",
    "outputId": "d20a84a4-0297-4c7e-f17d-75b9e00fb6af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 139        |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 3          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.4      |\n",
      "|    explained_variance | -0.219     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -83.6      |\n",
      "|    reward             | -0.2211089 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 5.52       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 140       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.5     |\n",
      "|    explained_variance | -0.0862   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -83.4     |\n",
      "|    reward             | 0.4700712 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 8.63      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 143        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 10         |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -75.6      |\n",
      "|    reward             | -2.3428001 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 12.2       |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 143          |\n",
      "|    iterations         | 400          |\n",
      "|    time_elapsed       | 13           |\n",
      "|    total_timesteps    | 2000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -41.5        |\n",
      "|    explained_variance | -1.19e-07    |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 399          |\n",
      "|    policy_loss        | -337         |\n",
      "|    reward             | 0.0002068775 |\n",
      "|    std                | 1.01         |\n",
      "|    value_loss         | 79.8         |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 141        |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 17         |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.6      |\n",
      "|    explained_variance | 0.0261     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | -312       |\n",
      "|    reward             | -1.2633901 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 60.9       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 141      |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 21       |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.5    |\n",
      "|    explained_variance | 0.0537   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | 52.4     |\n",
      "|    reward             | 2.059152 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 7.05     |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 140         |\n",
      "|    iterations         | 700         |\n",
      "|    time_elapsed       | 24          |\n",
      "|    total_timesteps    | 3500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.6       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 699         |\n",
      "|    policy_loss        | 19.2        |\n",
      "|    reward             | -0.14696413 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 1.46        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 139        |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 28         |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | 94.9       |\n",
      "|    reward             | 0.29835993 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 9.59       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 140       |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 32        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -56.6     |\n",
      "|    reward             | 1.1721771 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 4.33      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 141      |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 35       |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.5    |\n",
      "|    explained_variance | 0.0538   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | 191      |\n",
      "|    reward             | 4.175332 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 19.9     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 142       |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 38        |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.6     |\n",
      "|    explained_variance | -0.167    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | 27.4      |\n",
      "|    reward             | 2.1631613 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 3.28      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 141       |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 42        |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.6     |\n",
      "|    explained_variance | 0.0686    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | -6.85     |\n",
      "|    reward             | 1.0435524 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 0.398     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 141        |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 46         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.6      |\n",
      "|    explained_variance | -0.16      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | 14.7       |\n",
      "|    reward             | -0.5089326 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 4.82       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 141        |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 49         |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | 108        |\n",
      "|    reward             | -2.5177422 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 8          |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 142      |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 52       |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.6    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | -63      |\n",
      "|    reward             | 9.510674 |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 7.99     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 142       |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 56        |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.6     |\n",
      "|    explained_variance | -0.132    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | 83        |\n",
      "|    reward             | 3.3366086 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 6.61      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 142         |\n",
      "|    iterations         | 1700        |\n",
      "|    time_elapsed       | 59          |\n",
      "|    total_timesteps    | 8500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.7       |\n",
      "|    explained_variance | -0.000144   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1699        |\n",
      "|    policy_loss        | 170         |\n",
      "|    reward             | -0.37468588 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 21.2        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 143        |\n",
      "|    iterations         | 1800       |\n",
      "|    time_elapsed       | 62         |\n",
      "|    total_timesteps    | 9000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1799       |\n",
      "|    policy_loss        | 173        |\n",
      "|    reward             | -2.8031528 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 25.7       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 143        |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 66         |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | 93.1       |\n",
      "|    reward             | 0.24560197 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 7.66       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 144      |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 69       |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.7    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | -0.915   |\n",
      "|    reward             | 6.255911 |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 4.76     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 144       |\n",
      "|    iterations         | 2100      |\n",
      "|    time_elapsed       | 72        |\n",
      "|    total_timesteps    | 10500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.8     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2099      |\n",
      "|    policy_loss        | -243      |\n",
      "|    reward             | -3.505296 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 45        |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 144        |\n",
      "|    iterations         | 2200       |\n",
      "|    time_elapsed       | 76         |\n",
      "|    total_timesteps    | 11000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.7      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2199       |\n",
      "|    policy_loss        | 32.8       |\n",
      "|    reward             | -0.5989955 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 4.47       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 145        |\n",
      "|    iterations         | 2300       |\n",
      "|    time_elapsed       | 79         |\n",
      "|    total_timesteps    | 11500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.8      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2299       |\n",
      "|    policy_loss        | 140        |\n",
      "|    reward             | -0.1132694 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 12.2       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 145       |\n",
      "|    iterations         | 2400      |\n",
      "|    time_elapsed       | 82        |\n",
      "|    total_timesteps    | 12000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2399      |\n",
      "|    policy_loss        | 44.5      |\n",
      "|    reward             | 1.3633375 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 3.05      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 145        |\n",
      "|    iterations         | 2500       |\n",
      "|    time_elapsed       | 85         |\n",
      "|    total_timesteps    | 12500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2499       |\n",
      "|    policy_loss        | 64.5       |\n",
      "|    reward             | 0.18738079 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 15         |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 145        |\n",
      "|    iterations         | 2600       |\n",
      "|    time_elapsed       | 89         |\n",
      "|    total_timesteps    | 13000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2599       |\n",
      "|    policy_loss        | 54.9       |\n",
      "|    reward             | -4.1303687 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 5          |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 146        |\n",
      "|    iterations         | 2700       |\n",
      "|    time_elapsed       | 92         |\n",
      "|    total_timesteps    | 13500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.8      |\n",
      "|    explained_variance | 0.00699    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2699       |\n",
      "|    policy_loss        | -57.9      |\n",
      "|    reward             | -0.7231936 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 7.01       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 146        |\n",
      "|    iterations         | 2800       |\n",
      "|    time_elapsed       | 95         |\n",
      "|    total_timesteps    | 14000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.8      |\n",
      "|    explained_variance | 0.0023     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2799       |\n",
      "|    policy_loss        | -734       |\n",
      "|    reward             | -7.8988886 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 362        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 146        |\n",
      "|    iterations         | 2900       |\n",
      "|    time_elapsed       | 99         |\n",
      "|    total_timesteps    | 14500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2899       |\n",
      "|    policy_loss        | -11.5      |\n",
      "|    reward             | 0.69082314 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 6.26       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 146       |\n",
      "|    iterations         | 3000      |\n",
      "|    time_elapsed       | 102       |\n",
      "|    total_timesteps    | 15000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2999      |\n",
      "|    policy_loss        | -296      |\n",
      "|    reward             | 0.5311983 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 102       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 146       |\n",
      "|    iterations         | 3100      |\n",
      "|    time_elapsed       | 105       |\n",
      "|    total_timesteps    | 15500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.9     |\n",
      "|    explained_variance | 3.64e-06  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3099      |\n",
      "|    policy_loss        | 17.8      |\n",
      "|    reward             | -6.676799 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 18        |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 146        |\n",
      "|    iterations         | 3200       |\n",
      "|    time_elapsed       | 109        |\n",
      "|    total_timesteps    | 16000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.9      |\n",
      "|    explained_variance | 0.0557     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3199       |\n",
      "|    policy_loss        | 27.7       |\n",
      "|    reward             | -1.0032609 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 1.71       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 146        |\n",
      "|    iterations         | 3300       |\n",
      "|    time_elapsed       | 112        |\n",
      "|    total_timesteps    | 16500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3299       |\n",
      "|    policy_loss        | -54.6      |\n",
      "|    reward             | -3.4923146 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 2.38       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 146       |\n",
      "|    iterations         | 3400      |\n",
      "|    time_elapsed       | 115       |\n",
      "|    total_timesteps    | 17000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.9     |\n",
      "|    explained_variance | -0.234    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3399      |\n",
      "|    policy_loss        | 91.1      |\n",
      "|    reward             | 1.3713322 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 19.5      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 146       |\n",
      "|    iterations         | 3500      |\n",
      "|    time_elapsed       | 119       |\n",
      "|    total_timesteps    | 17500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.9     |\n",
      "|    explained_variance | -0.00907  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3499      |\n",
      "|    policy_loss        | 34.7      |\n",
      "|    reward             | -4.980754 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 3.28      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 146         |\n",
      "|    iterations         | 3600        |\n",
      "|    time_elapsed       | 123         |\n",
      "|    total_timesteps    | 18000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42         |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3599        |\n",
      "|    policy_loss        | -39.7       |\n",
      "|    reward             | -0.20942976 |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 2.65        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 146       |\n",
      "|    iterations         | 3700      |\n",
      "|    time_elapsed       | 126       |\n",
      "|    total_timesteps    | 18500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3699      |\n",
      "|    policy_loss        | -50.9     |\n",
      "|    reward             | 0.8652029 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 1.6       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 146       |\n",
      "|    iterations         | 3800      |\n",
      "|    time_elapsed       | 129       |\n",
      "|    total_timesteps    | 19000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3799      |\n",
      "|    policy_loss        | 170       |\n",
      "|    reward             | 2.2336557 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 17.7      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 145       |\n",
      "|    iterations         | 3900      |\n",
      "|    time_elapsed       | 133       |\n",
      "|    total_timesteps    | 19500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42       |\n",
      "|    explained_variance | 0.032     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3899      |\n",
      "|    policy_loss        | 60.3      |\n",
      "|    reward             | 1.8800225 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 3.23      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 145       |\n",
      "|    iterations         | 4000      |\n",
      "|    time_elapsed       | 137       |\n",
      "|    total_timesteps    | 20000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42       |\n",
      "|    explained_variance | 0.00619   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3999      |\n",
      "|    policy_loss        | 173       |\n",
      "|    reward             | 7.1002645 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 16        |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 145       |\n",
      "|    iterations         | 4100      |\n",
      "|    time_elapsed       | 140       |\n",
      "|    total_timesteps    | 20500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4099      |\n",
      "|    policy_loss        | 89.1      |\n",
      "|    reward             | 1.1778984 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 5.49      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 145        |\n",
      "|    iterations         | 4200       |\n",
      "|    time_elapsed       | 144        |\n",
      "|    total_timesteps    | 21000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.1      |\n",
      "|    explained_variance | 1.79e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4199       |\n",
      "|    policy_loss        | 61.6       |\n",
      "|    reward             | 0.25781375 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 4.67       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 145        |\n",
      "|    iterations         | 4300       |\n",
      "|    time_elapsed       | 147        |\n",
      "|    total_timesteps    | 21500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.1      |\n",
      "|    explained_variance | -0.0343    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4299       |\n",
      "|    policy_loss        | 57         |\n",
      "|    reward             | -5.5134296 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 3.7        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 145       |\n",
      "|    iterations         | 4400      |\n",
      "|    time_elapsed       | 150       |\n",
      "|    total_timesteps    | 22000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.2     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4399      |\n",
      "|    policy_loss        | -10.2     |\n",
      "|    reward             | 1.2680397 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 1.62      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 145       |\n",
      "|    iterations         | 4500      |\n",
      "|    time_elapsed       | 154       |\n",
      "|    total_timesteps    | 22500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.2     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4499      |\n",
      "|    policy_loss        | 90.9      |\n",
      "|    reward             | 5.9791794 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 40        |\n",
      "-------------------------------------\n",
      "day: 2514, episode: 110\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5533490.66\n",
      "total_reward: 4533490.66\n",
      "total_cost: 25440.32\n",
      "total_trades: 40078\n",
      "Sharpe: 1.088\n",
      "=================================\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 145        |\n",
      "|    iterations         | 4600       |\n",
      "|    time_elapsed       | 157        |\n",
      "|    total_timesteps    | 23000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4599       |\n",
      "|    policy_loss        | -122       |\n",
      "|    reward             | -2.0253317 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 8.56       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 145       |\n",
      "|    iterations         | 4700      |\n",
      "|    time_elapsed       | 161       |\n",
      "|    total_timesteps    | 23500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.3     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4699      |\n",
      "|    policy_loss        | -146      |\n",
      "|    reward             | -2.097497 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 13.4      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 145         |\n",
      "|    iterations         | 4800        |\n",
      "|    time_elapsed       | 164         |\n",
      "|    total_timesteps    | 24000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.3       |\n",
      "|    explained_variance | -0.0416     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 4799        |\n",
      "|    policy_loss        | -46.5       |\n",
      "|    reward             | -0.38498732 |\n",
      "|    std                | 1.04        |\n",
      "|    value_loss         | 2.38        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 145       |\n",
      "|    iterations         | 4900      |\n",
      "|    time_elapsed       | 168       |\n",
      "|    total_timesteps    | 24500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.3     |\n",
      "|    explained_variance | -0.00392  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4899      |\n",
      "|    policy_loss        | 150       |\n",
      "|    reward             | 7.2266035 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 22.7      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 145        |\n",
      "|    iterations         | 5000       |\n",
      "|    time_elapsed       | 171        |\n",
      "|    total_timesteps    | 25000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4999       |\n",
      "|    policy_loss        | 636        |\n",
      "|    reward             | -1.2077271 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 259        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 145        |\n",
      "|    iterations         | 5100       |\n",
      "|    time_elapsed       | 175        |\n",
      "|    total_timesteps    | 25500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5099       |\n",
      "|    policy_loss        | -0.973     |\n",
      "|    reward             | -1.5939938 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 0.208      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 145       |\n",
      "|    iterations         | 5200      |\n",
      "|    time_elapsed       | 178       |\n",
      "|    total_timesteps    | 26000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5199      |\n",
      "|    policy_loss        | 33.6      |\n",
      "|    reward             | 1.1645898 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 1.56      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 145       |\n",
      "|    iterations         | 5300      |\n",
      "|    time_elapsed       | 181       |\n",
      "|    total_timesteps    | 26500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.3     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5299      |\n",
      "|    policy_loss        | 127       |\n",
      "|    reward             | 1.5196629 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 11.3      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 145        |\n",
      "|    iterations         | 5400       |\n",
      "|    time_elapsed       | 185        |\n",
      "|    total_timesteps    | 27000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5399       |\n",
      "|    policy_loss        | -152       |\n",
      "|    reward             | 0.14473107 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 15         |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 145        |\n",
      "|    iterations         | 5500       |\n",
      "|    time_elapsed       | 188        |\n",
      "|    total_timesteps    | 27500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.3      |\n",
      "|    explained_variance | -0.0137    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5499       |\n",
      "|    policy_loss        | 38.6       |\n",
      "|    reward             | -6.6716475 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 41.6       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 145        |\n",
      "|    iterations         | 5600       |\n",
      "|    time_elapsed       | 192        |\n",
      "|    total_timesteps    | 28000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5599       |\n",
      "|    policy_loss        | 98.5       |\n",
      "|    reward             | 0.36087856 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 9.76       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 145        |\n",
      "|    iterations         | 5700       |\n",
      "|    time_elapsed       | 195        |\n",
      "|    total_timesteps    | 28500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5699       |\n",
      "|    policy_loss        | -116       |\n",
      "|    reward             | 0.13771492 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 12.8       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 145       |\n",
      "|    iterations         | 5800      |\n",
      "|    time_elapsed       | 198       |\n",
      "|    total_timesteps    | 29000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5799      |\n",
      "|    policy_loss        | -126      |\n",
      "|    reward             | 2.2945201 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 18.3      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 145       |\n",
      "|    iterations         | 5900      |\n",
      "|    time_elapsed       | 202       |\n",
      "|    total_timesteps    | 29500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.3     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5899      |\n",
      "|    policy_loss        | 41.8      |\n",
      "|    reward             | 1.6356832 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 1.54      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 145       |\n",
      "|    iterations         | 6000      |\n",
      "|    time_elapsed       | 206       |\n",
      "|    total_timesteps    | 30000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5999      |\n",
      "|    policy_loss        | 240       |\n",
      "|    reward             | 18.162184 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 53        |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 144        |\n",
      "|    iterations         | 6100       |\n",
      "|    time_elapsed       | 210        |\n",
      "|    total_timesteps    | 30500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6099       |\n",
      "|    policy_loss        | 11.9       |\n",
      "|    reward             | 0.19615528 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 0.211      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 144       |\n",
      "|    iterations         | 6200      |\n",
      "|    time_elapsed       | 214       |\n",
      "|    total_timesteps    | 31000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6199      |\n",
      "|    policy_loss        | 249       |\n",
      "|    reward             | 1.0686697 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 37.2      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 144        |\n",
      "|    iterations         | 6300       |\n",
      "|    time_elapsed       | 217        |\n",
      "|    total_timesteps    | 31500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.4      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6299       |\n",
      "|    policy_loss        | -211       |\n",
      "|    reward             | 0.34370548 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 31.8       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 144       |\n",
      "|    iterations         | 6400      |\n",
      "|    time_elapsed       | 221       |\n",
      "|    total_timesteps    | 32000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6399      |\n",
      "|    policy_loss        | -207      |\n",
      "|    reward             | 1.6093357 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 28.4      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 144        |\n",
      "|    iterations         | 6500       |\n",
      "|    time_elapsed       | 225        |\n",
      "|    total_timesteps    | 32500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6499       |\n",
      "|    policy_loss        | 266        |\n",
      "|    reward             | -0.8377509 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 53         |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 144      |\n",
      "|    iterations         | 6600     |\n",
      "|    time_elapsed       | 228      |\n",
      "|    total_timesteps    | 33000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.5    |\n",
      "|    explained_variance | -0.25    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6599     |\n",
      "|    policy_loss        | -93.7    |\n",
      "|    reward             | 1.470368 |\n",
      "|    std                | 1.05     |\n",
      "|    value_loss         | 7.37     |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 144         |\n",
      "|    iterations         | 6700        |\n",
      "|    time_elapsed       | 232         |\n",
      "|    total_timesteps    | 33500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.5       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6699        |\n",
      "|    policy_loss        | 40.8        |\n",
      "|    reward             | -0.39317888 |\n",
      "|    std                | 1.05        |\n",
      "|    value_loss         | 1.07        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 144       |\n",
      "|    iterations         | 6800      |\n",
      "|    time_elapsed       | 235       |\n",
      "|    total_timesteps    | 34000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | -0.000688 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6799      |\n",
      "|    policy_loss        | 19.7      |\n",
      "|    reward             | 4.445558  |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 1.2       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 144       |\n",
      "|    iterations         | 6900      |\n",
      "|    time_elapsed       | 239       |\n",
      "|    total_timesteps    | 34500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6899      |\n",
      "|    policy_loss        | 93.3      |\n",
      "|    reward             | -0.721137 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 7.34      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 143        |\n",
      "|    iterations         | 7000       |\n",
      "|    time_elapsed       | 243        |\n",
      "|    total_timesteps    | 35000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.5      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6999       |\n",
      "|    policy_loss        | -201       |\n",
      "|    reward             | -12.466117 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 30.9       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 143        |\n",
      "|    iterations         | 7100       |\n",
      "|    time_elapsed       | 246        |\n",
      "|    total_timesteps    | 35500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7099       |\n",
      "|    policy_loss        | 18         |\n",
      "|    reward             | 0.19026335 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 0.278      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 143        |\n",
      "|    iterations         | 7200       |\n",
      "|    time_elapsed       | 251        |\n",
      "|    total_timesteps    | 36000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7199       |\n",
      "|    policy_loss        | 9.31       |\n",
      "|    reward             | -3.4753408 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 0.766      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 143        |\n",
      "|    iterations         | 7300       |\n",
      "|    time_elapsed       | 255        |\n",
      "|    total_timesteps    | 36500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7299       |\n",
      "|    policy_loss        | 29.3       |\n",
      "|    reward             | 0.18075553 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 7.17       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 142       |\n",
      "|    iterations         | 7400      |\n",
      "|    time_elapsed       | 258       |\n",
      "|    total_timesteps    | 37000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | -0.00591  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7399      |\n",
      "|    policy_loss        | 358       |\n",
      "|    reward             | 2.5246084 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 99.8      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 142       |\n",
      "|    iterations         | 7500      |\n",
      "|    time_elapsed       | 262       |\n",
      "|    total_timesteps    | 37500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7499      |\n",
      "|    policy_loss        | 419       |\n",
      "|    reward             | 1.2186472 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 120       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 142         |\n",
      "|    iterations         | 7600        |\n",
      "|    time_elapsed       | 266         |\n",
      "|    total_timesteps    | 38000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.6       |\n",
      "|    explained_variance | -0.387      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 7599        |\n",
      "|    policy_loss        | -92.7       |\n",
      "|    reward             | 0.097186126 |\n",
      "|    std                | 1.05        |\n",
      "|    value_loss         | 4.91        |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 142      |\n",
      "|    iterations         | 7700     |\n",
      "|    time_elapsed       | 269      |\n",
      "|    total_timesteps    | 38500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.6    |\n",
      "|    explained_variance | -0.221   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7699     |\n",
      "|    policy_loss        | -64.8    |\n",
      "|    reward             | 1.733976 |\n",
      "|    std                | 1.05     |\n",
      "|    value_loss         | 3.31     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 142       |\n",
      "|    iterations         | 7800      |\n",
      "|    time_elapsed       | 273       |\n",
      "|    total_timesteps    | 39000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | -2.18     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7799      |\n",
      "|    policy_loss        | -39.9     |\n",
      "|    reward             | -2.977766 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 8.8       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 142         |\n",
      "|    iterations         | 7900        |\n",
      "|    time_elapsed       | 276         |\n",
      "|    total_timesteps    | 39500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.6       |\n",
      "|    explained_variance | -0.000362   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 7899        |\n",
      "|    policy_loss        | -50.2       |\n",
      "|    reward             | -0.85900927 |\n",
      "|    std                | 1.05        |\n",
      "|    value_loss         | 2.08        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 142       |\n",
      "|    iterations         | 8000      |\n",
      "|    time_elapsed       | 279       |\n",
      "|    total_timesteps    | 40000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | 0.0859    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7999      |\n",
      "|    policy_loss        | 286       |\n",
      "|    reward             | 2.7523658 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 59.8      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 142       |\n",
      "|    iterations         | 8100      |\n",
      "|    time_elapsed       | 283       |\n",
      "|    total_timesteps    | 40500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8099      |\n",
      "|    policy_loss        | -28.1     |\n",
      "|    reward             | -0.314721 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 0.55      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 142       |\n",
      "|    iterations         | 8200      |\n",
      "|    time_elapsed       | 286       |\n",
      "|    total_timesteps    | 41000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8199      |\n",
      "|    policy_loss        | 22        |\n",
      "|    reward             | 0.5375825 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 2.02      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 142       |\n",
      "|    iterations         | 8300      |\n",
      "|    time_elapsed       | 290       |\n",
      "|    total_timesteps    | 41500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8299      |\n",
      "|    policy_loss        | -9.03     |\n",
      "|    reward             | -0.660306 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 0.954     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 142        |\n",
      "|    iterations         | 8400       |\n",
      "|    time_elapsed       | 293        |\n",
      "|    total_timesteps    | 42000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8399       |\n",
      "|    policy_loss        | -22.1      |\n",
      "|    reward             | 0.10867972 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 0.537      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 142       |\n",
      "|    iterations         | 8500      |\n",
      "|    time_elapsed       | 297       |\n",
      "|    total_timesteps    | 42500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8499      |\n",
      "|    policy_loss        | -309      |\n",
      "|    reward             | 15.481132 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 81.1      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 142         |\n",
      "|    iterations         | 8600        |\n",
      "|    time_elapsed       | 300         |\n",
      "|    total_timesteps    | 43000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.7       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 8599        |\n",
      "|    policy_loss        | 2.9         |\n",
      "|    reward             | -0.29810843 |\n",
      "|    std                | 1.06        |\n",
      "|    value_loss         | 0.0703      |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 142       |\n",
      "|    iterations         | 8700      |\n",
      "|    time_elapsed       | 304       |\n",
      "|    total_timesteps    | 43500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8699      |\n",
      "|    policy_loss        | 53.7      |\n",
      "|    reward             | 1.2004697 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 2.13      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 143       |\n",
      "|    iterations         | 8800      |\n",
      "|    time_elapsed       | 307       |\n",
      "|    total_timesteps    | 44000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.8     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8799      |\n",
      "|    policy_loss        | 11.2      |\n",
      "|    reward             | -3.689428 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 0.689     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 143       |\n",
      "|    iterations         | 8900      |\n",
      "|    time_elapsed       | 311       |\n",
      "|    total_timesteps    | 44500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8899      |\n",
      "|    policy_loss        | -76.1     |\n",
      "|    reward             | 5.3552213 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 3.9       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 143         |\n",
      "|    iterations         | 9000        |\n",
      "|    time_elapsed       | 314         |\n",
      "|    total_timesteps    | 45000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.8       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 8999        |\n",
      "|    policy_loss        | -83.8       |\n",
      "|    reward             | -0.30050364 |\n",
      "|    std                | 1.06        |\n",
      "|    value_loss         | 30.5        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 143       |\n",
      "|    iterations         | 9100      |\n",
      "|    time_elapsed       | 317       |\n",
      "|    total_timesteps    | 45500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.8     |\n",
      "|    explained_variance | 0.439     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9099      |\n",
      "|    policy_loss        | -57.9     |\n",
      "|    reward             | 1.9673392 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 1.95      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 143        |\n",
      "|    iterations         | 9200       |\n",
      "|    time_elapsed       | 321        |\n",
      "|    total_timesteps    | 46000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9199       |\n",
      "|    policy_loss        | 34.4       |\n",
      "|    reward             | -0.4544251 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 2.17       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 143         |\n",
      "|    iterations         | 9300        |\n",
      "|    time_elapsed       | 324         |\n",
      "|    total_timesteps    | 46500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.9       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 9299        |\n",
      "|    policy_loss        | 69.8        |\n",
      "|    reward             | -0.08694011 |\n",
      "|    std                | 1.06        |\n",
      "|    value_loss         | 2.92        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 143       |\n",
      "|    iterations         | 9400      |\n",
      "|    time_elapsed       | 328       |\n",
      "|    total_timesteps    | 47000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9399      |\n",
      "|    policy_loss        | 157       |\n",
      "|    reward             | 2.4883509 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 13.8      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 143        |\n",
      "|    iterations         | 9500       |\n",
      "|    time_elapsed       | 331        |\n",
      "|    total_timesteps    | 47500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9499       |\n",
      "|    policy_loss        | 115        |\n",
      "|    reward             | -0.8363399 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 19.7       |\n",
      "--------------------------------------\n",
      "day: 2514, episode: 120\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3341277.10\n",
      "total_reward: 2341277.10\n",
      "total_cost: 6772.49\n",
      "total_trades: 36069\n",
      "Sharpe: 0.999\n",
      "=================================\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 143        |\n",
      "|    iterations         | 9600       |\n",
      "|    time_elapsed       | 335        |\n",
      "|    total_timesteps    | 48000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43        |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9599       |\n",
      "|    policy_loss        | -30.8      |\n",
      "|    reward             | -0.4585423 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 3.34       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 143        |\n",
      "|    iterations         | 9700       |\n",
      "|    time_elapsed       | 338        |\n",
      "|    total_timesteps    | 48500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9699       |\n",
      "|    policy_loss        | -161       |\n",
      "|    reward             | 0.51702744 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 22.7       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 143       |\n",
      "|    iterations         | 9800      |\n",
      "|    time_elapsed       | 341       |\n",
      "|    total_timesteps    | 49000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9799      |\n",
      "|    policy_loss        | 145       |\n",
      "|    reward             | 1.0114391 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 13.1      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 143       |\n",
      "|    iterations         | 9900      |\n",
      "|    time_elapsed       | 345       |\n",
      "|    total_timesteps    | 49500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9899      |\n",
      "|    policy_loss        | -91.5     |\n",
      "|    reward             | -3.384957 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 5.02      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 143       |\n",
      "|    iterations         | 10000     |\n",
      "|    time_elapsed       | 348       |\n",
      "|    total_timesteps    | 50000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9999      |\n",
      "|    policy_loss        | -319      |\n",
      "|    reward             | 1.3755068 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 86.1      |\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "trained_a2c = agent.train_model(model=model_a2c,\n",
    "                             tb_log_name='a2c',\n",
    "                             total_timesteps=50000) if if_using_a2c else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1730813629062,
     "user": {
      "displayName": "Natty Metekie",
      "userId": "06571578933818431366"
     },
     "user_tz": -240
    },
    "id": "yw89A8hkf_0E",
    "outputId": "5e849271-97f5-4475-82d7-f93d02ffdc96"
   },
   "outputs": [],
   "source": [
    "trained_a2c.save(config.TRAINED_MODEL_DIR + \"/agent_a2c\") if if_using_a2c else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fpsb_srwxSeJ"
   },
   "source": [
    "Trading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "pbhgA-S2xSeJ"
   },
   "outputs": [],
   "source": [
    "e_trade_gym = StockTradingEnv(df = trade, **env_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 501,
     "status": "ok",
     "timestamp": 1730813629552,
     "user": {
      "displayName": "Natty Metekie",
      "userId": "06571578933818431366"
     },
     "user_tz": -240
    },
    "id": "YKsJ4NO1xSeJ",
    "outputId": "e89a8649-9096-4fbe-ff5f-0f516d3b1578"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hit end!\n"
     ]
    }
   ],
   "source": [
    "df_account_value, df_actions = DRLAgent.DRL_prediction(\n",
    "    model=trained_a2c,\n",
    "    environment = e_trade_gym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 261
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1730813629553,
     "user": {
      "displayName": "Natty Metekie",
      "userId": "06571578933818431366"
     },
     "user_tz": -240
    },
    "id": "32rJegIHxSeK",
    "outputId": "773a6dcb-7f45-4886-a773-9a4f3b3c6a14"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>account_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>2020-09-23</td>\n",
       "      <td>1.012342e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>2020-09-24</td>\n",
       "      <td>1.015620e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>2020-09-25</td>\n",
       "      <td>1.022471e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>2020-09-28</td>\n",
       "      <td>1.032990e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>2020-09-29</td>\n",
       "      <td>1.026637e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  account_value\n",
       "183  2020-09-23   1.012342e+06\n",
       "184  2020-09-24   1.015620e+06\n",
       "185  2020-09-25   1.022471e+06\n",
       "186  2020-09-28   1.032990e+06\n",
       "187  2020-09-29   1.026637e+06"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_account_value.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SViEqNgCxSeK"
   },
   "source": [
    "Backtesting Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1730813629553,
     "user": {
      "displayName": "Natty Metekie",
      "userId": "06571578933818431366"
     },
     "user_tz": -240
    },
    "id": "Ojev0r2pxSeK",
    "outputId": "dc6c8e32-0ae1-4652-fdb6-f5725557fcee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============Get Backtest Results===========\n",
      "Annual return          0.035866\n",
      "Cumulative returns     0.026637\n",
      "Annual volatility      0.366240\n",
      "Sharpe ratio           0.279592\n",
      "Calmar ratio           0.116998\n",
      "Stability              0.087663\n",
      "Max drawdown          -0.306548\n",
      "Omega ratio            1.057756\n",
      "Sortino ratio          0.392271\n",
      "Skew                        NaN\n",
      "Kurtosis                    NaN\n",
      "Tail ratio             0.998670\n",
      "Daily value at risk   -0.045736\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"==============Get Backtest Results===========\")\n",
    "now = datetime.datetime.now().strftime('%Y%m%d-%Hh%M')\n",
    "\n",
    "perf_stats_all = backtest_stats(account_value=df_account_value)\n",
    "perf_stats_all = pd.DataFrame(perf_stats_all)\n",
    "perf_stats_all.to_csv(\"./\"+config.RESULTS_DIR+\"/perf_stats_all_\"+now+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1730813629553,
     "user": {
      "displayName": "Natty Metekie",
      "userId": "06571578933818431366"
     },
     "user_tz": -240
    },
    "id": "2bZAHxflxSeK",
    "outputId": "e44cced7-eb0d-41bf-f35f-03b9ba1f2063"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============Get Baseline Stats===========\n",
      "Shape of DataFrame:  (188, 8)\n",
      "Annual return         -0.065199\n",
      "Cumulative returns    -0.049054\n",
      "Annual volatility      0.416030\n",
      "Sharpe ratio           0.046016\n",
      "Calmar ratio          -0.175803\n",
      "Stability              0.012240\n",
      "Max drawdown          -0.370862\n",
      "Omega ratio            1.009343\n",
      "Sortino ratio          0.062829\n",
      "Skew                        NaN\n",
      "Kurtosis                    NaN\n",
      "Tail ratio             0.860019\n",
      "Daily value at risk   -0.052339\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#baseline stats\n",
    "print(\"==============Get Baseline Stats===========\")\n",
    "baseline_df = get_baseline(\n",
    "        ticker=\"^DJI\",\n",
    "        start = '2020-01-01',\n",
    "        end = '2020-09-30')\n",
    "\n",
    "stats = backtest_stats(baseline_df, value_col_name = 'close')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 509
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1730813629553,
     "user": {
      "displayName": "Natty Metekie",
      "userId": "06571578933818431366"
     },
     "user_tz": -240
    },
    "id": "NjpzZeMJxSeK",
    "outputId": "c9d1ae3b-170f-4fe1-d5c1-4a637e76a191"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a2c</th>\n",
       "      <th>dji</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-01-02</th>\n",
       "      <td>1.000000e+06</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-03</th>\n",
       "      <td>9.988238e+05</td>\n",
       "      <td>1.000000e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-06</th>\n",
       "      <td>9.993823e+05</td>\n",
       "      <td>1.002392e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-07</th>\n",
       "      <td>9.984936e+05</td>\n",
       "      <td>9.982119e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-08</th>\n",
       "      <td>1.001921e+06</td>\n",
       "      <td>1.003848e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-23</th>\n",
       "      <td>1.012342e+06</td>\n",
       "      <td>9.346322e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-24</th>\n",
       "      <td>1.015620e+06</td>\n",
       "      <td>9.364587e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-25</th>\n",
       "      <td>1.022471e+06</td>\n",
       "      <td>9.489818e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-28</th>\n",
       "      <td>1.032990e+06</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-29</th>\n",
       "      <td>1.026637e+06</td>\n",
       "      <td>9.587147e+05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>188 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     a2c           dji\n",
       "date                                  \n",
       "2020-01-02  1.000000e+06           NaN\n",
       "2020-01-03  9.988238e+05  1.000000e+06\n",
       "2020-01-06  9.993823e+05  1.002392e+06\n",
       "2020-01-07  9.984936e+05  9.982119e+05\n",
       "2020-01-08  1.001921e+06  1.003848e+06\n",
       "...                  ...           ...\n",
       "2020-09-23  1.012342e+06  9.346322e+05\n",
       "2020-09-24  1.015620e+06  9.364587e+05\n",
       "2020-09-25  1.022471e+06  9.489818e+05\n",
       "2020-09-28  1.032990e+06           NaN\n",
       "2020-09-29  1.026637e+06  9.587147e+05\n",
       "\n",
       "[188 rows x 2 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result_a2c = df_account_value.set_index(df_account_value.columns[0])\n",
    "result = pd.DataFrame(\n",
    "    {\n",
    "        \"a2c\": df_result_a2c[\"account_value\"],\n",
    "        \"dji\": dji[\"close\"],\n",
    "    }\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1730813629553,
     "user": {
      "displayName": "Natty Metekie",
      "userId": "06571578933818431366"
     },
     "user_tz": -240
    },
    "id": "Opz_OgDbxSeK",
    "outputId": "cd988cd3-4020-4414-8903-d8a1ff7b6c15"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_45767/4043962701.py:3: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (15,5)\n",
    "plt.figure()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1730813629553,
     "user": {
      "displayName": "Natty Metekie",
      "userId": "06571578933818431366"
     },
     "user_tz": -240
    },
    "id": "K7P5-LrIzmuz",
    "outputId": "f61fd5f8-bf52-415e-af07-9df423777404"
   },
   "outputs": [],
   "source": [
    "result.plot()\n",
    "plt.savefig('results_a2c.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "finrl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
